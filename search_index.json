[["index.html", "Tidy Modeling with R Book Club Welcome", " Tidy Modeling with R Book Club The R4DS Online Learning Community 2021-02-05 Welcome This is a companion for the book Tidy Modeling with R by Max Kuhn and Julia Silge. This companion is available at r4ds.io/tmwr. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book. This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["software-for-modeling.html", "Chapter 1 Software for modeling", " Chapter 1 Software for modeling Learning objectives: Recognize the principles around which the {tidymodels} packages were designed. Classify models as descriptive, inferential, and/or predictive. Define descriptive model. Define inferential model. Define predictive model. Differentiate between supervised and unsupervised models. Differentiate between regression and classification models. Differentiate between quantitative and qualitative data. Understand the roles that data can have in an analysis. Apply the data science process. Recognize the phases of modeling. The utility of a model hinges on its ability to be reductive. The primary influences in the data can be captured mathematically in a useful way, such as in a relationship that can be expressed as an equation. There are two reasons that models permeate our lives today: an abundance of software exists to create models and it has become easier to record data and make it accessible. "],["the-pit-of-success.html", "1.1 The pit of success", " 1.1 The pit of success {tidymodels} aims to help us fall into the Pit of Success: The Pit of Success: in stark contrast to a summit, a peak, or a journey across a desert to find victory through many trials and surprises, we want our customers to simply fall into winning practices by using our platform and frameworks. Avoid confusion: Software should facilitate proper usage. Avoid mistakes: Software should make it easy for users to do the right thing. "],["types-of-models.html", "1.2 Types of models", " 1.2 Types of models Descriptive models: Describe or illustrate characteristics of data. Inferential models: Make some statement of truth regarding a predefined conjecture or idea. Inferential techniques typically produce some type of probabilistic output, such as a p-value, confidence interval, or posterior probability. Usually delayed feedback between inference and actual result. Predictive models: Produce the most accurate possible prediction for new data. Estimation (“How much?”) rather than inference (“Will it?”). Mechanistic models are derived using first principles to produce a model equation that is dependent on assumptions. Depend on the assumptions that define their model equations. Unlike inferential models, it is easy to make data-driven statements about how well the model performs based on how well it predicts the existing data Empirically driven models have more vague assumptions, and are derived directly from the data. No theoretical or probabilistic assumptions are made about the equations or the variables The primary method of evaluating the appropriateness of the model is to assess its accuracy using existing data 1. Broader discussions of these distinctions can be found in Breiman (2001b) and Shmueli (2010) "],["terminology.html", "1.3 Terminology", " 1.3 Terminology Unsupervised models are used to understand relationships between variables or sets of variables without an explicit relationship between variables and an outcome. Examples: PCA, clustering, autoencoders. Supervised models have an outcome variable. Examples: linear regression, neural networks. Regression: numeric outcome Classification: ordered or unordered qualitative values. Quantitative data: numbers. Qualitative (nominal) data: non-numbers. Data can have different roles in analyses: Outcomes (labels, endpoints, dependent variables): the value being predicted in supervised models. Predictors (independent variables): the variables used to predict the outcome. "],["the-data-analysis-process.html", "1.4 The data analysis process", " 1.4 The data analysis process Cleaning the data: investigate the data to make sure that they are applicable to the project goals, accurate, and appropriate Understanding the data: often referred to as exploratory data analysis (EDA). EDA brings to light how the different variables are related to one another, their distributions, typical ranges, and other attributes. “How did I come by these data?” “Is the data relevant?” Develop clear expectations of the goal of your model and how performance will be judged (Chapter 9) “What is/are the performance metrics or realistic goal/s of what can be achieved?” The data science process (from R for Data Science by Wickham and Grolemund. "],["the-modeling-process.html", "1.5 The modeling process", " 1.5 The modeling process The modeling process. Exploratory data analysis: Explore the data to see what they might tell you. (See previous) Feature engineering: Create specific model terms that make it easier to accurately model the observed data. Covered in Chapter 6. Model tuning and selection: Generate a variety of models and compare performance. Some models require hyperparameter tuning Model evaluation: Use EDA-like analyses and compare model performance metrics to choose the best model for your situation. "],["meeting-videos.html", "1.6 Meeting Videos", " 1.6 Meeting Videos 1.6.1 Cohort 1 Meeting chat log 00:10:57 Andrew G. Farina: Sorry guys, I have a sleeping baby in the room, so I am stuck with only chat tonight. Looking forward to the discussion though. 00:11:10 mayagans: Hi baby!!! 00:11:31 mayagans: (Hi everyone else too — Im also in a loud house right now super stoked for this!) 00:11:38 Jim Gruman: Hello everyone 00:11:39 Tony ElHabr: the chat is where all of the fun happens anyways! 00:11:59 Tan Ho: Obviously! 00:12:01 Scott Nestler: It&#39;s been way too long since I&#39;ve seen many of you. Hope everyone is doing well. I&#39;m excited for this. 00:12:04 Jeremy: Yep, I’ve got a puppy who believes she’s an attack dog going crazy so I’ll probably mute for a while 00:12:36 Tyler Grant Smith: on kid bath duty for the start of this 00:15:16 Yoni Sidi: It’s a gitbook! 00:15:37 Tan Ho: It&#39;s a book about a book 00:15:40 Tan Ho: classic Jon 00:15:46 Joe Sydlowski: Metabook 00:15:54 Scott Nestler: Very meta. 00:16:03 Tony ElHabr: presentation + book seems like it is prime for a package 00:16:17 Tony ElHabr: counting on jon to jump on that idea 00:26:44 shamsuddeen: The utility of a model hinges on its ability to be reductive. What is the meaning of this from the book? 00:28:03 Tony ElHabr: I think that means &quot;a model should be interpretable&quot; 00:28:28 Tony ElHabr: yeah, &quot;simpler&quot; is a better word 00:28:41 Yoni Sidi: Sparse model means less overfitting 00:28:41 shamsuddeen: sure 00:29:15 Gabriela Palomo: Perhaps it may also mean that a model uses a bunch of data and simplifies it in an equation or model? 00:29:29 Jacob Miller: As someone who is an intermediate user of caret, how useful would it be to switch completely over to tidymodels and not revert back to caret? Or are there benefits to using both consistently? 00:29:56 Gabriela Palomo: So in a way it&#39;s simpler to understand as well vs seeing all the raw data 00:30:09 Tony ElHabr: i feel you have much more &quot;low-level&quot; control with tidymodels 00:30:36 Scott Nestler: I had a similar question to Jacob&#39;s, but with regard to mle &amp; mle3. 00:31:01 Tan Ho: Caret is broader but tidymodels is deeper (see yesterday&#39;s xkcd :P) 00:31:10 Arjun’s iPhone: you can mix tidymodels and caret.... preprocess using tidymodels and feed it to caret 00:32:12 Scott Nestler: TYPO ALERT. I meant mlr and mlr3. 00:32:18 Asmae Toumi: Agreed with David. For example, weighted RMSE stuff is only on caret (for now) and there’s a GitHub issue reply by max basically saying its too hard to add to tidy models right now. Either way tidy models seems the way to go to not be behind in say, 2 years, when its well developed 00:32:20 Conor Tompkins: My understanding is that caret is deprecated. It still works, but tidymodels is where its at now. Like dplyr in 2015. Not 100% coverage compared to base R or data.table, but heading in the right direction fast. 00:32:25 Maria: Yes, usemodels is great! 00:32:38 Conor Tompkins: Yeah not officially depreacted 00:34:30 mayagans: My only comment is that I love how many people are here!!!! I can only imagine the range of domain expertise in this “room” - I HATE ice breakers but do people want to throw in the chat what domain they want to write models in/why they’re reading this book? Im a pharma person but Im also obsessed with music analytics :) I look forward to seeing how presenters apply their chapters! 00:34:43 Connor Krenzer: The book says in the Empirically driven models section: &quot;No theoretical or probabilistic assumptions are made about the sales numbers or the variables that are used to define similarity. In fact, the primary method of evaluating the appropriateness of the model is to assess its accuracy using existing data. If the structure of this type of model was a good choice, the predictions would be close to the actual values.&quot; How does the significance of the model&#39;s variables play into the above? Let&#39;s take linear regression for example. Does this mean we are only supposed to care about R-square instead of p-values? 00:34:51 Yoni Sidi: https://github.com/topepo/workflowsets 00:35:41 Asmae Toumi: Sure Maya, great idea, my domain is healthcare/medtech and for fun, sports analytics! 00:36:44 Jordan Krogmann: Live and die by the pun :) 00:37:04 Kevin Kent: @Maya - I originally learned ML stuff in sklearn but do 80% of my work in R, so I’d like to move that all over to tidy models. I work in healthcare technology, in the devops area 00:37:26 Tony ElHabr: good question Connor. i think it depends on your intention. for an inferential model, the variables and p values matter more, but that&#39;s not to say that your model&#39;s R2 is &quot;allowed&quot; to be really bad. for a predictive model, it would all be about maximizing R2 00:37:38 Yoni Sidi: Modeling and simulation in pharma 00:37:59 Conor Tompkins: I don’t use modeling professionally, would like to get there. I use R to avoid using Excel. Very interested in sports data and civic hacking 00:38:12 Scott Nestler: I guide many students in capstone projects building models in all kinds of domains. Much of my own work is in sports analytics, either for fun or with some of our teams here on campus. 00:38:24 Tony ElHabr: electricity markets. sports for fun 00:38:36 Jonathan Trattner: Undergrad studying computational neuroscience! 00:38:43 Jonathan Trattner: I can volunteer for the tidyverse primer! 00:38:45 Jim Gruman: Im in industry/agriculture - marketing/geospatial/IoT events/survival 00:38:46 Maria: I also in healthcare/research 00:38:58 Tyler Grant Smith: im a predictive modeling actuary working in p&amp;c insurance 00:39:02 Vasant M: @Connor Krenzer yes, the less you use p-values as a metric to assess models the better. R-square is one metric, but not always the most reliable one to use. For instance r-square doesn’t mean anything for non-linear models. I would rather depend on model accuracy to guide model building 00:39:05 Jonathan Trattner: Sounds good (: 00:39:08 Tan Ho: I work in homebuilding, so finance-ish data - and work on fantasy football data as well 00:39:13 Stephen - Computer - No Mic: Degree in Health Data Analytics - currently working on an automated trading algorithm (which is built with tidymodels) 00:39:29 Aashish Cheruvu: I’m a student and I’m interesting in healthcare analytics and tech 00:39:38 Miles Ott (he/him/his): Hi everyone! Excited to be here :)I am a stats/data science prof at Smith College and my work/research stuff is in social network analysis and sampling applied to public health 00:39:43 Vasant M: I am Bioinformatician - Work in Biomedical research, currently doing Lipidomics in Sleep Mediccine 00:39:51 shamsuddeen: Student interested in natural language processing 00:39:55 Andrew G. Farina: I am a grad candidate currently, trying to build a solid base in modeling to use in the future. 00:39:56 Stephen - Computer - No Mic: I have been using R to run a text messaging campaign for the Senate run-offs in Georgia recently 00:40:13 Tim Moloney: I work in environmental consulting, do a lot of geospatial and/or statistics analyses with R 00:40:19 Adrienne St Clair: Hi all, I&#39;m a botanist and work in plant conservation in public parks. I am a nascent data nerd and want to learn all I can about data analysis. 00:40:21 Conor Tompkins: I am currently using tidymodels to build a model to predict house sale prices in Pittsburgh 00:40:34 Jonathan Leslie: I work in data science consulting...I work with businesses/government agencies to design data science projects. 00:40:43 Vasant M: @Stephen that’s very cool. 00:40:45 ErickKnackstedt: Business intelligence developer in the mental health/mindfulness space, no real modeling experience really excited to learn tidymodels 00:40:53 Andrew G (he/him): I work in App Analytics. Will be starting a new gig in app/game analytics soon. Historically modeling on the job has been few and far between so I’m looking forward to understanding best practices, workflow, etc… 00:41:23 Ben Gramza: Hi I&#39;m Ben, I just graduated with a stats degree (and thus am unemployed and without a domain). I&#39;ve done some work with COVID survey data and redistricting/gerrymandering in the past. I also keep up with the sports analytics scene in my free time. 00:41:53 Giovani Ferreira: Tech Team Leader here, data hobbyist, usually very interested in NLP and Topic Modelling, decided to use this bookclub to level my modelling skills 00:42:25 Jacob Miller: Senior studying stats, done actuarial consulting internships, and planning on grad school in stats. Sports analytics is the hobby/passion 00:43:01 mayagans: Aaaahhh so many cool domains!! Everyone is a bad ass wow - I selfishly hope everyone talks ties in the content with their passions and maybe Ill even know something about #SPORTS by the time we’re done LOL 00:44:00 Stephen - Computer - No Mic: Thanks @ Vasant ! 00:44:21 Conor Tompkins: Deployment seems very domain specific 00:44:38 Conor Tompkins: Tech stack = domain 00:45:57 David Severski: Oh, do I have thoughts on cloudy… ;P 00:46:04 David Severski: S/cloudy/cloudyr/ 00:46:32 Jordan Krogmann: https://github.com/wlandau/targets 00:46:39 tim: To get some more background in machine learning, in addition to learning tidymodels, any suggestions for books? I was thinking Applied Predictive Modeling - but keep changing my mind and need something to stick to. I guess it uses caret too? So that might be useful. 00:47:36 mayagans: ……Is Yoni in an aquarium of pizzas? 00:48:07 Tan Ho: asking the important questions :D 00:48:16 Scott Nestler: Responding to Tim&#39;s question … I&#39;m currently working my way through Machine Learning with R, the tidyverse, and mlr (Rhys). 00:48:25 Vasant M: @Tim Statistical Learning PDF link http://www.ime.unicamp.br/~dias/Intoduction%20to%20Statistical%20Learning.pdf 00:48:34 Connor Krenzer: @tim I hear Introduction to Statistical Learning is a classic 00:49:06 Vasant M: @Tim if you like a course https://www.edx.org/course/statistical-learning 00:49:15 Tony ElHabr: i&#39;m just glad we won&#39;t have to nag people to volunteer to do presentations since we have so many participants lol 00:49:21 ErickKnackstedt: https://dtkaplan.github.io/SM2-bookdown/preface-to-this-electronic-version.html 00:49:38 tim: Thanks, this is awesome! Now I just need to pick something and stick with it, haha 00:49:38 ErickKnackstedt: That book is legit 00:49:59 Jonathan Leslie: @Tim I second the recommendation for Introduction to Statistical Learning. It’s a great overview of different modelling approaches and how to interpret model outputs. 00:50:18 Miles Ott (he/him/his): nice to meet you all! 00:50:19 David Severski: Have a great one, everyone! 00:50:24 Jordan Krogmann: thanks take it ea y 00:50:28 Yoni Sidi: Bye and thanks for all the fish 00:50:29 Tan Ho: Cheers gang! 00:50:32 Aashish Cheruvu: Bye everyone and thank you 00:50:33 mayagans: Thanks Jon!! 00:50:36 Maria: Cheers! 00:51:03 Arjun’s iPhone: p "],["a-tidyverse-primer.html", "Chapter 2 A tidyverse primer", " Chapter 2 A tidyverse primer Learning objectives: List the tidyverse design principles. Explain what it means for the tidyverse to be designed for humans. Describe how reusing existing data structures can make functions easier to work with. Explain what it means for a set of functions to be designed for the pipe. Explain what it means for function to be designed for functional programming. List some differences between a tibble and a base data.frame. Recognize how to use the tidyverse to read and wrangle data. "],["tidyverse-design-principles.html", "2.1 Tidyverse design Principles", " 2.1 Tidyverse design Principles The tidyverse has four core design principles: Human centered: Designed to promote human usability. Consistent: Learning how to use one function or package is as similar as another. Composable: Easily breakdown data challenges into smaller components with exploratory tools to find the best solution. Inclusive: Fostering a community of like-minded users (e.g. #rstats) "],["design-for-humans.html", "2.2 Design for Humans", " 2.2 Design for Humans “Programs must be written for people to read, and only incidentally for machines to execute.” - Hal Abelson The tidyverse offers packages that are easily readable and understood by humans. It enables them to more easily achieve their programming goals. Consider the mtcars dataset, which comprises fuel consumption and 10 aspects of autombile design and performance from 1973-1974. Previewing the first six rows of the data, we see: ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 If we wanted to arrange these in ascending order based on the mpg and gear variables, how could we do this? The function arrange(), in the dplyr package of the tidyverse, takes a data frame and column names as such: arrange(.data = mtcars, gear, mpg) arrange(), and other tidyverse functions, use names that are descriptive and explicit. For general methods, there is a focus on verbs, as seen with the functions pivot_longer() and pivot_wider() in the tidyr package. "],["reusing-existing-data-structures.html", "2.3 Reusing existing data structures", " 2.3 Reusing existing data structures “You don’t have to reinvent the wheel, just attach it to a new wagon.” - Mark McCormack There are many different data types in R, such as matrices, lists, and data frames.1 A typical function would take in data of some form, conduct an operation, and return the result. tidyverse functions most often operate on data structures called tibbles. Traditional data frames can represent different data types in each column, and multiple values in each row. Tibbles are a special data frame that have additional properties helpful for data analysis. Example: list-columns boot_samp &lt;- rsample::bootstraps(mtcars, times = 3) boot_samp ## # Bootstrap sampling ## [90m# A tibble: 3 x 2[39m ## splits id ## [3m[90m&lt;list&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m ## [90m1[39m [90m&lt;split [32/15]&gt;[39m Bootstrap1 ## [90m2[39m [90m&lt;split [32/12]&gt;[39m Bootstrap2 ## [90m3[39m [90m&lt;split [32/15]&gt;[39m Bootstrap3 class(boot_samp) ## [1] &quot;bootstraps&quot; &quot;rset&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; The above example shows how to create bootstrap resamples of the data frame mtcars. It returns a tibble with a splits column that defines the resampled data sets. This function inherits data frame and tibble methods so other functions that operate on those data structures can be used. For a more detailed discussion, see Hadley Wickham’s Advanced R↩︎ "],["designed-for-the-pipe.html", "2.4 Designed for the pipe", " 2.4 Designed for the pipe The pipe operator, %&gt;%, comes from the magrittr package by Stefan Milton Bache, and is used to chain together a sequence of R functions. More specifically, the pipe operator uses the value of the object on the left-hand side of the operator as the first argument on the operator’s right-hand side. The pipe allows for highly readable code. Consider wanting to sort the mtcars dataset by the number of gears (gear) and then select the first ten rows. How would you do that? cars_arranged &lt;- arrange(mtcars, gear) cars_selected &lt;- slice(cars_arranged, 1:10) # more compactly cars_selected &lt;- slice(arrange(mtcars, gear), 1:10) Using the pipe to substitute the left-hand side of the operator with the first argument on the right-hand side, we can get the same result as follows: cars_selected &lt;- mtcars %&gt;% arrange(gear) %&gt;% slice(1:10) This approach with the pipe works because all the functions return the same data structure (a tibble/data frame) which is the first argument of the next function. Whenever possible, create functions that can be incorporated into a pipeline of operations. "],["designed-for-functional-programming.html", "2.5 Designed for Functional Programming", " 2.5 Designed for Functional Programming Functional Programming is an approach to replace iterative (i.e. for) loops. Consider the case where you may want two times the square root of the mpg for each car in mtcars. You could do this with a for loop as follows: n &lt;- nrow(mtcars) roots &lt;- rep(NA_real_, n) for (car in 1:n) { roots[car] &lt;- 2 * sqrt(mtcars$mpg[car]) } You could also write a function to do the computations. In functional programming, it’s important that the function does not have any side effects and the output only depends on the inputs. For example, the function my_sqrt() takes in a car’s mpg and a weight by which to multiply the square root. my_sqrt &lt;- function(mpg, weight) { weight * sqrt(mpg) } Using the purrr package, we can forgo the for loop and use the map() family of functions which use the basic syntax of map(vector, function). Below, we are applying the my_sqrt() function, with a weight of 2, to the first three elements of mtcars$mpg. User supplied functions can be declared by prefacing it with ~ (pronounced “twiddle”). By default, map() returns a list. If you know the class of a function’s output, you can use special suffixes. A character output, for example, would used by map_chr(), a double by map_dbl(), and a logical by map_lgl(). map( .x = head(mtcars$mpg, 3), ~ my_sqrt( mpg = .x, weight = 2 ) ) ## [[1]] ## [1] 9.165151 ## ## [[2]] ## [1] 9.165151 ## ## [[3]] ## [1] 9.549869 map() functions can be used with 2 inputs, by specifying map2() Requires arguments .x and .y map2( .x = head(mtcars$mpg, 3), .y = c(1,2,3), ~ my_sqrt( mpg = .x, weight = .y ) ) ## [[1]] ## [1] 4.582576 ## ## [[2]] ## [1] 9.165151 ## ## [[3]] ## [1] 14.3248 "],["tibbles-vs-data-frames.html", "2.6 Tibbles vs. Data Frames", " 2.6 Tibbles vs. Data Frames A tibble is a special type of data frame with some additional properties. Specifically: Tibbles work with column names that are not syntactically valid variable names. data.frame(`this does not work` = 1:2, oops = 3:4) ## this.does.not.work oops ## 1 1 3 ## 2 2 4 tibble(`this does work, though` = 1:2, `woohoo!` = 3:4) ## [90m# A tibble: 2 x 2[39m ## `this does work, though` `woohoo!` ## [3m[90m&lt;int&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m ## [90m1[39m 1 3 ## [90m2[39m 2 4 Tibbles prevent partial matching of arguments to avoid accidental errors df &lt;- data.frame(partial = 1:5) tbbl &lt;- tibble(partial = 1:5) df$part ## [1] 1 2 3 4 5 tbbl$part ## Warning: Unknown or uninitialised column: `part`. ## NULL Tibbles prevent dimension dropping, so subsetting data into a single column will never return a vector. df[, &quot;partial&quot;] ## [1] 1 2 3 4 5 tbbl[, &quot;partial&quot;] ## [90m# A tibble: 5 x 1[39m ## partial ## [3m[90m&lt;int&gt;[39m[23m ## [90m1[39m 1 ## [90m2[39m 2 ## [90m3[39m 3 ## [90m4[39m 4 ## [90m5[39m 5 Tibbles allow for list-columns, which can be a powerful tool when working with the purrr package. template_list &lt;- list(a = 1, b = 2, c = 3, d = 4, e = 5) data.frame(col = 1:5, list_col = template_list) ## col list_col.a list_col.b list_col.c list_col.d list_col.e ## 1 1 1 2 3 4 5 ## 2 2 1 2 3 4 5 ## 3 3 1 2 3 4 5 ## 4 4 1 2 3 4 5 ## 5 5 1 2 3 4 5 tibble(col = 1:5, list_col = template_list) ## [90m# A tibble: 5 x 2[39m ## col list_col ## [3m[90m&lt;int&gt;[39m[23m [3m[90m&lt;named list&gt;[39m[23m ## [90m1[39m 1 [90m&lt;dbl [1]&gt;[39m ## [90m2[39m 2 [90m&lt;dbl [1]&gt;[39m ## [90m3[39m 3 [90m&lt;dbl [1]&gt;[39m ## [90m4[39m 4 [90m&lt;dbl [1]&gt;[39m ## [90m5[39m 5 [90m&lt;dbl [1]&gt;[39m "],["how-to-read-and-wrangle-data.html", "2.7 How to read and wrangle data", " 2.7 How to read and wrangle data The following example shows how to use the tidyverse to read in data (with the readr package) and easily manipulate it (using the dplyr and lubridate packages). We will walk through these steps during our meeting. library(tidyverse) library(lubridate) url &lt;- &quot;http://bit.ly/raw-train-data-csv&quot; all_stations &lt;- # Step 1: Read in the data. readr::read_csv(url) %&gt;% # Step 2: filter columns and rename stationname dplyr::select(station = stationname, date, rides) %&gt;% # Step 3: Convert the character date field to a date encoding. # Also, put the data in units of 1K rides dplyr::mutate(date = lubridate::mdy(date), rides = rides / 1000) %&gt;% # Step 4: Summarize the multiple records using the maximum. dplyr::group_by(date, station) %&gt;% dplyr::summarize(rides = max(rides), .groups = &quot;drop&quot;) head(all_stations, 10) ## [90m# A tibble: 10 x 3[39m ## date station rides ## [3m[90m&lt;date&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m 2001-01-01 18th 0 ## [90m 2[39m 2001-01-01 35-Bronzeville-IIT 0.448 ## [90m 3[39m 2001-01-01 35th/Archer 0.318 ## [90m 4[39m 2001-01-01 43rd 0.211 ## [90m 5[39m 2001-01-01 47th-Dan Ryan 0.787 ## [90m 6[39m 2001-01-01 47th-South Elevated 0.427 ## [90m 7[39m 2001-01-01 51st 0.364 ## [90m 8[39m 2001-01-01 54th/Cermak 0 ## [90m 9[39m 2001-01-01 63rd-Dan Ryan 1.37 ## [90m10[39m 2001-01-01 69th 2.37 “This pipeline of operations illustrates why the tidyverse is popular. A series of data manipulations is used that have simple and easy to understand user interfaces; the series is bundled together in a streamlined and readable way. The focus is on how the user interacts with the software. This approach enables more people to learn R and achieve their analysis goals, and adopting these same principles for modeling in R has the same benefits.” - Max Kuhn and Julia Silge in Tidy Modeling with R "],["meeting-videos-1.html", "2.8 Meeting Videos", " 2.8 Meeting Videos 2.8.1 Cohort 1 Meeting chat log 00:07:43 Daniel Chen: hi Jon. hi everyone. first time here :) 00:08:03 Jon Harmon (jonthegeek): Welcome! 00:09:27 KakShA Ekam DallasSK: the gang is here..except for tyler 00:09:31 KakShA Ekam DallasSK: and his kid 00:10:01 David Severski: The 4 seasons of hex bin stickers. LOL 00:10:11 Tan Ho: MiniTyler doesn&#39;t need help with the tidyverse 00:10:44 KakShA Ekam DallasSK: that&#39;s right..he just wanted to learn bangbang 00:11:11 Tan Ho: hey, there&#39;s Tyler! 00:11:23 pavitra: yay!! 00:12:36 Daniel Chen: i feel like more people have cameras on and respond here than any other zoom meeting i&#39;ve been in all year... 00:12:50 Tony ElHabr: haha I second that 00:13:03 Tan Ho: we&#39;re all frens here :) 00:17:25 Tony ElHabr: i will not hate on using mtcars i will not hate on using mtcars i will not hate on using mtcars 00:17:30 Tan Ho: Ooh! A use case for the &lt;details&gt; block 00:17:41 Tan Ho: shush, no dataset snobbery 00:17:54 Darya Vanichkina: Yup, details is so helpful for easily hiding stuff in Rmd 00:18:06 Jon Harmon (jonthegeek): Ooooh, good call on details, let&#39;s make it clear how to do that in the instructions! (I&#39;m still partly here). 00:18:25 Yoni Sidi: Shameless plug {details} https://github.com/yonicd/details 00:18:48 Tan Ho: ohhh dang 00:18:53 Tan Ho: installing now, I use that all the time 00:19:24 mayagans: Ooooooh it makes a collapsible div?? That is sick nasty! 00:19:29 pavitra: wow, details is very cool 00:19:34 David Severski: Funny, I was just exposed to details in GH issues earlier today. :) 00:20:03 Yoni Sidi: Seamless integration to rmd with it’s own knitter chunk engine…ok I’m done plugging 00:20:15 Vasant M: Ah yes details! That’s why I recall your name @yoni Sidi 00:20:31 Tony ElHabr: lol i have also been having this weird data frame print out issue 00:20:55 Darya Vanichkina: I wonder why… 00:21:27 Tan Ho: must be new tibble package related issue 00:22:17 Yoni Sidi: What print problem? 00:22:22 Daniel Chen: fyi: not all things that work on tibbles will work on data.frame 00:22:36 Tony ElHabr: things like &quot;[90m&quot; get printed out instead of &quot;#&quot; 00:22:53 Gabriela Palomo: and viceversa data.frame -&gt; tibbles 00:22:55 Tan Ho: https://r4ds.github.io/bookclub-tmwr/reusing-existing-data-structures.html 00:23:39 Tony ElHabr: jonathan brought the jokes tonight 00:23:42 Tony ElHabr: always love that 00:23:47 Jon Harmon (jonthegeek): Thinking of %&gt;% as &quot;and then&quot; is very useful. Learning to read code in a way that makes sense is super super helpful! 00:24:24 Yoni Sidi: Ironically Tony You can look at the print method for data.frames in details to see how I got around it 00:24:38 Tony ElHabr: hmm will do Yoni 00:25:08 Tyler Grant Smith: for small loops and especially if youve preallocated the for loop is probably faster 00:25:10 Daniel Chen: to fix the &quot;[90m&quot; stuff you need to put: options(crayon.enabled = FALSE) either in your renvion or on the top of the knited file at the 00:25:25 Yoni Sidi: :+1: 00:25:36 Tony ElHabr: thx y&#39;all. will try that out! 00:26:03 Darya Vanichkina: To evaluate the speed you can use microbenchmark::microbenchmark(&lt;code here&gt;) - I tend to use it when picking from stackoverflow 00:26:23 Tan Ho: ope TIL that&#39;s called a twiddle 00:26:28 Vasant M: i thought ~ is tilde 00:26:34 Jon Harmon (jonthegeek): bench::mark is another option (by Jim Hester) 00:26:38 Tan Ho: today-I-learned 00:27:03 Daniel Chen: yes it&#39;s an annononyoms fuunction 00:27:05 Vasant M: Statistician here and I call it tilde 00:27:07 Tan Ho: I always called it a lambda 00:27:09 Daniel Chen: the twittle is a shortcut for the lambda 00:27:27 Tan Ho: Hadley renames things outta nowhere for no reason 00:27:28 Tan Ho: grah 00:27:32 Jon Harmon (jonthegeek): Good to hear that, Vasant! But it isn&#39;t just Hadley! 00:27:34 Tan Ho: insert pooh meme 00:27:49 mayagans: And twittle is writing a model in twitter 00:27:51 Vasant M: Ya! I think it’s the atlantic/pacific divide 00:28:01 Tony ElHabr: zoom really missed the boat by not allowing images to be embedded in chat 00:28:08 Darya Vanichkina: .x is also hidden from the global environment (i.e. it exists but is not visible in the environment, from memory (?) 00:28:11 Jon Harmon (jonthegeek): https://hsm.stackexchange.com/questions/7999/why-do-mathematicians-call-twiddle#:~:text=Tilde%20is%20from%20Spanish%20tildar,pretty%20apt%20name%20for%20it. 00:28:30 mayagans: Im still trying to find the JB quote about loops 00:28:32 Tyler Grant Smith: i prefer giving arguments in the ... to map when possible rather than making a partial lambda. opinions? 00:28:33 Yoni Sidi: Wait … isn’t ~ $/sim$? 00:28:59 Darya Vanichkina: Tyler, can you give an example? 00:29:05 Tan Ho: &gt; JB quote about loops someone has to write a for loop, but it doesn&#39;t have to be you! 00:29:13 mayagans: Lol yes thank you tan so good 00:29:20 Tony ElHabr: yea tyler. ellipses as the default, and partial only minimally 00:29:57 Tan Ho: yeah, where possible I write map, map-args, function, static args, but not always possible with argument order 00:30:04 Tyler Grant Smith: so for the given example. map(1:5, my_sqrt, weight = 2) 00:30:25 Tan Ho: tbh Tyler started it 00:32:15 Kevin Kent: Yeah I like the … static arg style too. 00:32:54 Darya Vanichkina: And if we have more than one arg would it be (ex_ weight = 2, height = 3, randomarg = 4)? 00:33:03 Daniel Chen: to tyler, re ... : the ... works, but you can&#39;t map a vector of values with it (can&#39;t do: map(1:5, my_sqrt, weight = 11:15) ). that&#39;s what map2 and pmap are for. 00:33:16 Tan Ho: LIVE CODE 00:33:18 Tan Ho: YAY 00:33:26 Joe Sydlowski: The correct window setup too! 00:33:29 Tony ElHabr: insert environment pane comment 00:33:32 Tan Ho: BOOO 00:33:40 Tan Ho: BOOOOOOOOO 00:33:41 Tyler Grant Smith: yep daniel 00:34:05 Tyler Grant Smith: and Darya, yes 00:34:09 David Severski: Source goes on the right. Console on left. Change my mind. ;P 00:34:17 Tan Ho: oh nooooo 00:34:18 mayagans: Thats chaos 00:34:25 atoumi: this is absolutely the correct layout 00:34:30 mayagans: ^^^ 00:34:32 Tan Ho: where&#39;s daryn I need backup 00:34:34 Darya Vanichkina: @atoumi I agree 00:34:36 mayagans: Queen has spoken 00:34:37 Kevin Kent: haha. My eyes would get crossed with source on the right 00:34:40 Tyler Grant Smith: that hurts my brain to think about 00:34:40 Daniel Chen: i have to say it&#39;s the &quot;wrong env pane setup&quot; when you&#39;re teaching (new learners) becuase students ask &quot;mine don&#39;t look like that&quot;. unless you take out time from the lesson to show them how to customize it 00:34:57 Darya Vanichkina: @Daniel I actually do that at the start of every workshop 00:35:13 David Severski: Now it’s time for the RMarkdown notebook output to console/in-line debate. :D 00:35:26 Darya Vanichkina: Because that allows me to stick EVERYTHING into the bottom right pane, and have source take up the entire left of the screen 00:35:28 Tan Ho: i assume we don&#39;t need to have a lightmode darkmode fight? 00:35:39 Tony ElHabr: i think we are all on the dark side, right? 00:35:43 atoumi: Output to console and darkmode because we care about our eyes 00:35:44 Scott Nestler: That might be something we all agree on. 00:35:46 Tony ElHabr: RIGHT Y&#39;ALL? 00:35:47 Darya Vanichkina: Also, I think it’s a discussion, not a fight … 00:35:59 Tan Ho: shush 00:36:00 David Severski: For teaching, Studio Cloud or a dockerized version is my goto setup. 00:36:08 Tan Ho: we don&#39;t pull punches with dark mode 00:36:50 David Severski: Yay! Rsthemes! 00:36:53 mayagans: Blindeddd by the lightttt 00:37:07 Darya Vanichkina: I’m Carpentries-trained/based, so I still tend to want people to walk away with something they can work with on their own data later 00:37:11 Darya Vanichkina: Although it does take time 00:37:21 Tony ElHabr: makes sense Darya 00:37:40 Daniel Chen: i run into horizontal space issues when i make the font bigger when i have to teach, but that usually ends up being workshop dependent. 00:37:43 Joe Sydlowski: I never noticed that before! 00:38:25 Darya Vanichkina: Surprisingly, I recently had a horrific workshop (geospatial python with DL, via zoom - install issues every session), and the post-workshop survey learners STILL said they wanted us to fix installs (instead of a docker-based VM solution) 00:38:43 Darya Vanichkina: [I am still setting up a viable docker/VM for next year] 00:39:11 Daniel Chen: for python, can you use a binder instance for workshops? 00:39:27 Darya Vanichkina: I needed GPUs 00:39:33 Daniel Chen: :( 00:40:00 Daniel Chen: conda environment.yml files aren&#39;t able to install the binaries? 00:40:28 Connor Krenzer: Could you use the map() function on this dataset? 00:40:29 mayagans: Your python talk is gonna get you booted from this chat (lol jk xoxo) 00:40:32 David Severski: I kinda hate the warning messages the new dplyr spits out about groups (yes, I know they’re configurable). 00:40:34 Jordan Krogmann: Great job! 00:40:37 Tony ElHabr: discussion of python install issues in an R meeting? how fitting 00:40:54 Yoni Sidi: Great job Jonathan! 00:40:54 Tan Ho: i threw the option into my rprofile once and have never had problems again 00:41:02 Darya Vanichkina: @mayagans/@Tony I think it’s more a teaching issue 00:41:08 Darya Vanichkina: And I definitely use/teach both 00:41:10 Tony ElHabr: yeah, global Rprofile option ftw 00:41:19 Tony ElHabr: don&#39;t get me wrong, i love python too 00:41:26 Tony ElHabr: i just like to joke haha 00:41:28 David Severski: I do to, but I use a lot of renv these days, so I’m always going into environments where my rprofile isn’t active 00:45:14 Vasant M: Especially when you apply a lot of different functions (models) , being explicit is better 00:45:19 Tan Ho: https://imgur.com/WdcWnKz 00:45:54 Connor Krenzer: Is there a reason why you would use map() instead of vapply()? 00:46:52 Daniel Chen: vapply lets you specify the output types and how many elements in each list element. map only returns a list. 00:47:28 Daniel Chen: that&#39;s map vs map_* map will return a list 00:48:25 David Severski: And the easy path to parallelization with `furrr` is so nice. 00:49:01 Darya Vanichkina: @David agreed 00:49:56 Yoni Sidi: https://github.com/hrbrmstr/freebase 00:51:02 David Severski: He really called it freebase? :P 00:51:06 Darya Vanichkina: Thanks, everyone! 00:51:08 David Severski: Bye! "],["a-review-of-r-modeling-fundamentals.html", "Chapter 3 A review of R modeling fundamentals", " Chapter 3 A review of R modeling fundamentals Learning objectives: Specify model terms using the R formula syntax. List conveniences for modeling that are supported by the R formula syntax. Use anova() to compare models. Use summary() to inspect a model. Use predict() to generate new predictions from a model. List the three purposes that the R model formula serves. Recognize how the design for humans rubric is applied to {tidymodels} packages. Use broom::tidy() to standardize the structure of R objects. Use the {tidyverse} along with base modeling functions like lm() to produce multiple models at once. "],["r-formula-syntax.html", "3.1 R formula syntax", " 3.1 R formula syntax We’ll use the trees data set provided in {modeldata} (loaded with {tidymodels}) for demonstration purposes. Tree girth (in inches), height (in feet), and volume (in cubic feet) are provided. (Girth is somewhat like a measure of diameter.) library(tidyverse) library(tidymodels) theme_set(theme_minimal(base_size = 14)) data(trees) trees &lt;- as_tibble(trees) trees ## [90m# A tibble: 31 x 3[39m ## Girth Height Volume ## [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m 8.3 70 10.3 ## [90m 2[39m 8.6 65 10.3 ## [90m 3[39m 8.8 63 10.2 ## [90m 4[39m 10.5 72 16.4 ## [90m 5[39m 10.7 81 18.8 ## [90m 6[39m 10.8 83 19.7 ## [90m 7[39m 11 66 15.6 ## [90m 8[39m 11 75 18.2 ## [90m 9[39m 11.1 80 22.6 ## [90m10[39m 11.2 75 19.9 ## [90m# … with 21 more rows[39m Note that there is an analytical way to calculate tree volume from measures of diameter and height. We observe that Girth is strongly correlated with Volume trees %&gt;% corrr::correlate() ## [90m# A tibble: 3 x 4[39m ## term Girth Height Volume ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m Girth [31mNA[39m 0.519 0.967 ## [90m2[39m Height 0.519 [31mNA[39m 0.598 ## [90m3[39m Volume 0.967 0.598 [31mNA[39m Shame on you 😉 if you didn’t guess I would make a scatter plot given a data set with two variables. trees %&gt;% ggplot(aes(x = Girth, y = Height)) + geom_point(aes(size = Volume)) We can fit a linear regression model to predict Volume as a function of the other two features, using the formula syntax to save us from some typing. reg_fit &lt;- lm(Volume ~ ., data = trees) reg_fit ## ## Call: ## lm(formula = Volume ~ ., data = trees) ## ## Coefficients: ## (Intercept) Girth Height ## -57.9877 4.7082 0.3393 How would you write this without the formula syntax? If we want to get fancy with our pipes (%&gt;%), then we should wrap our formula with formula() trees %&gt;% lm(formula(Volume ~ .), data = .) ## ## Call: ## lm(formula = formula(Volume ~ .), data = .) ## ## Coefficients: ## (Intercept) Girth Height ## -57.9877 4.7082 0.3393 Interaction terms are easy to generate. inter_fit &lt;- lm(Volume ~ Girth * Height, data = trees) inter_fit ## ## Call: ## lm(formula = Volume ~ Girth * Height, data = trees) ## ## Coefficients: ## (Intercept) Girth Height Girth:Height ## 69.3963 -5.8558 -1.2971 0.1347 Same goes for polynomial terms. poly_fit &lt;- lm(Volume ~ Girth + I(Girth^2) + Height, data = trees) poly_fit ## ## Call: ## lm(formula = Volume ~ Girth + I(Girth^2) + Height, data = trees) ## ## Coefficients: ## (Intercept) Girth I(Girth^2) Height ## -9.9204 -2.8851 0.2686 0.3764 Excluding columns is intuitive. no_height_fit &lt;- lm(Volume ~ . - Height, data = trees) no_height_fit ## ## Call: ## lm(formula = Volume ~ . - Height, data = trees) ## ## Coefficients: ## (Intercept) Girth ## -36.943 5.066 The intercept term can be removed conveniently. no_intercept_fit &lt;- lm(Volume ~ . + 0, data = trees) no_intercept_fit ## ## Call: ## lm(formula = Volume ~ . + 0, data = trees) ## ## Coefficients: ## Girth Height ## 5.0440 -0.4773 To illustrate another convenience provided by formulas, let’s add a categorical column. trees2 &lt;- trees set.seed(42) trees2$group = sample(toupper(letters[1:4]), size = nrow(trees2), replace = TRUE) trees2 ## [90m# A tibble: 31 x 4[39m ## Girth Height Volume group ## [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m ## [90m 1[39m 8.3 70 10.3 A ## [90m 2[39m 8.6 65 10.3 A ## [90m 3[39m 8.8 63 10.2 A ## [90m 4[39m 10.5 72 16.4 A ## [90m 5[39m 10.7 81 18.8 B ## [90m 6[39m 10.8 83 19.7 D ## [90m 7[39m 11 66 15.6 B ## [90m 8[39m 11 75 18.2 B ## [90m 9[39m 11.1 80 22.6 A ## [90m10[39m 11.2 75 19.9 D ## [90m# … with 21 more rows[39m Encoding the categories as separate features is done auto-magically with the formula syntax. dummy_fit &lt;- lm(Volume ~ ., data = trees2) dummy_fit ## ## Call: ## lm(formula = Volume ~ ., data = trees2) ## ## Coefficients: ## (Intercept) Girth Height groupB groupC groupD ## -55.2921 4.6932 0.3093 -1.8367 -0.0497 0.6462 Under the hood, this is done by model.matrix(). model.matrix(Volume ~ ., data = trees2) %&gt;% head(10) ## (Intercept) Girth Height groupB groupC groupD ## 1 1 8.3 70 0 0 0 ## 2 1 8.6 65 0 0 0 ## 3 1 8.8 63 0 0 0 ## 4 1 10.5 72 0 0 0 ## 5 1 10.7 81 1 0 0 ## 6 1 10.8 83 0 0 1 ## 7 1 11.0 66 1 0 0 ## 8 1 11.0 75 1 0 0 ## 9 1 11.1 80 0 0 0 ## 10 1 11.2 75 0 0 1 3.1.1 Recap Purposes of R model formula: The formula defines the columns that are used by the model. The standard R machinery uses the formula to encode the columns into an appropriate format. The roles of the columns are defined by the formula. "],["inspecting-and-developing-models.html", "3.2 Inspecting and developing models", " 3.2 Inspecting and developing models Being the sound analysts that we are, we should check if the assumptions of linear regression are violated. The plot() generic function has a specific method for lm objects that generates various diagnostic plots. par(mfrow = c(1, 2)) plot(reg_fit, which = c(1, 2)) The second plot does not show any strong violation of the normality assumption. However, the first plot shows a violation of the linearity assumption (that there is a linear relationship between the response variable and the predictors). If the assumption were satisfied, the smooth red line would be like a straight horizontal line at y=0. Note that there is a {ggplot2} way to generate the same plots. library(ggfortify) autoplot(reg_fit, which = c(1, 2)) But what about the coefficients? summary(reg_fit) ## ## Call: ## lm(formula = Volume ~ ., data = trees) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.4065 -2.6493 -0.2876 2.2003 8.4847 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -57.9877 8.6382 -6.713 2.75e-07 *** ## Girth 4.7082 0.2643 17.816 &lt; 2e-16 *** ## Height 0.3393 0.1302 2.607 0.0145 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.882 on 28 degrees of freedom ## Multiple R-squared: 0.948, Adjusted R-squared: 0.9442 ## F-statistic: 255 on 2 and 28 DF, p-value: &lt; 2.2e-16 Use {broom} for a tidy version. library(broom) reg_fit %&gt;% tidy() ## [90m# A tibble: 3 x 5[39m ## term estimate std.error statistic p.value ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m (Intercept) -[31m58[39m[31m.[39m[31m0[39m 8.64 -[31m6[39m[31m.[39m[31m71[39m 2.75[90me[39m[31m- 7[39m ## [90m2[39m Girth 4.71 0.264 17.8 8.22[90me[39m[31m-17[39m ## [90m3[39m Height 0.339 0.130 2.61 1.45[90me[39m[31m- 2[39m reg_fit %&gt;% glance() %&gt;% glimpse() ## Rows: 1 ## Columns: 12 ## $ r.squared [3m[90m&lt;dbl&gt;[39m[23m 0.94795 ## $ adj.r.squared [3m[90m&lt;dbl&gt;[39m[23m 0.9442322 ## $ sigma [3m[90m&lt;dbl&gt;[39m[23m 3.881832 ## $ statistic [3m[90m&lt;dbl&gt;[39m[23m 254.9723 ## $ p.value [3m[90m&lt;dbl&gt;[39m[23m 1.071238e-18 ## $ df [3m[90m&lt;dbl&gt;[39m[23m 2 ## $ logLik [3m[90m&lt;dbl&gt;[39m[23m -84.45499 ## $ AIC [3m[90m&lt;dbl&gt;[39m[23m 176.91 ## $ BIC [3m[90m&lt;dbl&gt;[39m[23m 182.6459 ## $ deviance [3m[90m&lt;dbl&gt;[39m[23m 421.9214 ## $ df.residual [3m[90m&lt;int&gt;[39m[23m 28 ## $ nobs [3m[90m&lt;int&gt;[39m[23m 31 {purrr} and {dplyr} can help you scale up your modeling process. We can compare all of the models we made before. list( &#39;reg&#39; = reg_fit, &#39;inter&#39; = inter_fit, &#39;poly&#39; = poly_fit, &#39;no_height&#39; = no_height_fit, &#39;no_intercept&#39; = no_intercept_fit ) %&gt;% map_dfr(glance, .id = &#39;id&#39;) %&gt;% select(id, adj.r.squared) %&gt;% arrange(desc(adj.r.squared)) ## [90m# A tibble: 5 x 2[39m ## id adj.r.squared ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m poly 0.975 ## [90m2[39m inter 0.973 ## [90m3[39m no_intercept 0.968 ## [90m4[39m reg 0.944 ## [90m5[39m no_height 0.933 We observe that the polynomial fit is the best. We can create models for each group in trees2. reg_fits &lt;- trees2 %&gt;% group_nest(group) %&gt;% mutate( fit = map(data, ~ lm(formula(Volume ~ .), data = .x)), tidied = map(fit, tidy), glanced = map(fit, glance), augmented = map(fit, augment) ) .select_unnest &lt;- function(data, ...) { data %&gt;% select(group, ...) %&gt;% unnest(...) } reg_fits %&gt;% .select_unnest(tidied) ## [90m# A tibble: 12 x 6[39m ## group term estimate std.error statistic p.value ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m A (Intercept) -[31m44[39m[31m.[39m[31m6[39m 17.5 -[31m2[39m[31m.[39m[31m55[39m 0.031[4m2[24m ## [90m 2[39m A Girth 4.21 0.477 8.83 0.000[4m0[24m[4m0[24m[4m9[24m98 ## [90m 3[39m A Height 0.250 0.294 0.849 0.418 ## [90m 4[39m B (Intercept) -[31m66[39m[31m.[39m[31m1[39m 13.9 -[31m4[39m[31m.[39m[31m74[39m 0.017[4m8[24m ## [90m 5[39m B Girth 4.16 0.704 5.91 0.009[4m6[24m[4m9[24m ## [90m 6[39m B Height 0.520 0.123 4.24 0.024[4m0[24m ## [90m 7[39m C (Intercept) -[31m86[39m[31m.[39m[31m4[39m 90.5 -[31m0[39m[31m.[39m[31m954[39m 0.410 ## [90m 8[39m C Girth 4.83 0.747 6.47 0.007[4m4[24m[4m8[24m ## [90m 9[39m C Height 0.680 1.20 0.567 0.611 ## [90m10[39m D (Intercept) -[31m46[39m[31m.[39m[31m3[39m 14.8 -[31m3[39m[31m.[39m[31m14[39m 0.034[4m9[24m ## [90m11[39m D Girth 6.03 0.372 16.2 0.000[4m0[24m[4m8[24m[4m5[24m2 ## [90m12[39m D Height -[31m0[39m[31m.[39m[31m0[39m[31m26[4m8[24m[39m 0.214 -[31m0[39m[31m.[39m[31m125[39m 0.906 reg_fits %&gt;% .select_unnest(glanced) ## [90m# A tibble: 4 x 13[39m ## group r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m A 0.960 0.951 3.81 107. 5.27[90me[39m[31m-7[39m 2 -[31m31[39m[31m.[39m[31m3[39m 70.7 72.6 ## [90m2[39m B 0.935 0.891 2.20 21.5 1.66[90me[39m[31m-2[39m 2 -[31m11[39m[31m.[39m[31m2[39m 30.3 29.5 ## [90m3[39m C 0.946 0.910 4.06 26.2 1.26[90me[39m[31m-2[39m 2 -[31m14[39m[31m.[39m[31m8[39m 37.7 36.8 ## [90m4[39m D 0.990 0.985 2.80 194. 1.04[90me[39m[31m-4[39m 2 -[31m15[39m[31m.[39m[31m2[39m 38.4 38.2 ## [90m# … with 3 more variables: deviance [3m[90m&lt;dbl&gt;[90m[23m, df.residual [3m[90m&lt;int&gt;[90m[23m, nobs [3m[90m&lt;int&gt;[90m[23m[39m reg_fits %&gt;% .select_unnest(augmented) ## [90m# A tibble: 31 x 10[39m ## group Volume Girth Height .fitted .resid .hat .sigma .cooksd .std.resid ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m A 10.3 8.3 70 7.81 2.49 0.252 3.91 0.064[4m5[24m 0.758 ## [90m 2[39m A 10.3 8.6 65 7.82 2.48 0.283 3.90 0.078[4m0[24m 0.769 ## [90m 3[39m A 10.2 8.8 63 8.17 2.03 0.395 3.93 0.103 0.687 ## [90m 4[39m A 16.4 10.5 72 17.6 -[31m1[39m[31m.[39m[31m18[39m 0.134 4.01 0.005[4m6[24m[4m6[24m -[31m0[39m[31m.[39m[31m332[39m ## [90m 5[39m A 22.6 11.1 80 22.1 0.500 0.534 4.03 0.014[4m2[24m 0.192 ## [90m 6[39m A 19.1 12 75 24.6 -[31m5[39m[31m.[39m[31m54[39m 0.123 3.45 0.113 -[31m1[39m[31m.[39m[31m56[39m ## [90m 7[39m A 22.2 12.9 74 28.2 -[31m5[39m[31m.[39m[31m99[39m 0.083[4m7[24m 3.38 0.082[4m3[24m -[31m1[39m[31m.[39m[31m64[39m ## [90m 8[39m A 36.3 14.5 74 34.9 1.37 0.116 4.00 0.006[4m4[24m[4m3[24m 0.383 ## [90m 9[39m A 38.3 16 72 40.8 -[31m2[39m[31m.[39m[31m45[39m 0.330 3.90 0.101 -[31m0[39m[31m.[39m[31m787[39m ## [90m10[39m A 55.7 17.5 82 49.6 6.13 0.255 3.16 0.397 1.87 ## [90m# … with 21 more rows[39m "],["more-of-base-and-stats.html", "3.3 More of {base} and {stats}", " 3.3 More of {base} and {stats} R’s {base} and {stats} libraries have lots of built-in functions that help perform statistical analysis. For example, anova() can be used to compare two regression models quickly. anova(reg_fit, poly_fit) ## Analysis of Variance Table ## ## Model 1: Volume ~ Girth + Height ## Model 2: Volume ~ Girth + I(Girth^2) + Height ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 28 421.92 ## 2 27 186.01 1 235.91 34.243 3.13e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We observe that the second order term for Girth does indeed provide significant explanatory power to the model. (Formally, we reject the null hypothesis that the second order term for Girth is zero.) What is ANOVA? Use base R statistical function when someone tries to test your statistics knowledge. Question: If \\(U_1\\) and \\(U_2\\) are i.i.d. \\(Unif(0,1)\\) random variables, what is the distribution of \\(U_1 + U_2\\)? set.seed(42) n &lt;- 10000 u_1 &lt;- runif(n) u_2 &lt;- runif(n) .hist &lt;- function(x, ...) { hist(x, probability = TRUE,...) lines(density(x), col = &quot;blue&quot;, lwd = 2, ...) } layout(matrix(c(1,2,3,3), 2, 2, byrow = TRUE)) .hist(u_1) .hist(u_2) .hist(u_1 + u_2) Answer: Evidently it’s triangular. There are probably lots of functions that you didn’t know you even needed. add_column &lt;- function(data) { # Whoops! `df` should be `data` df %&gt;% mutate(dummy = 1) } trees %&gt;% add_column() ## Error in UseMethod(&quot;mutate&quot;): no applicable method for &#39;mutate&#39; applied to an object of class &quot;function&quot; df() is the density function for the F distribution with df1 and df2 degrees of freedom df ## function (x, df1, df2, ncp, log = FALSE) ## { ## if (missing(ncp)) ## .Call(C_df, x, df1, df2, log) ## else .Call(C_dnf, x, df1, df2, ncp, log) ## } ## &lt;bytecode: 0x7fac699e5b68&gt; ## &lt;environment: namespace:stats&gt; "],["why-tidy-principles-and-tidymodels.html", "3.4 Why Tidy Principles and {tidymodels}?", " 3.4 Why Tidy Principles and {tidymodels}? The {tidyverse} has four guiding principles which {tidymodels} shares. It is human centered, i.e. the {tidyverse} is designed specifically to support the activities of a human data analyst. Functions use sensible defaults, or use no defaults in cases where the user must make a choice (e.g. a file path). {recipes} and {parnsip} enable data frames to be used every where in the modeling process. Data frames are often more convenient than working with matrices/vectors. It is consistent, so that what you learn about one function or package can be applied to another, and the number of special cases that you need to remember is as small as possible. Object orientated programming (mainly S3) for functions such as predict() provide a consistent interface to the user. broom::tidy() output is in a consistent format (data frame). List outputs provided by package-specific functions vary. It is composable, allowing you to solve complex problems by breaking them down into small pieces, supporting a rapid cycle of exploratory iteration to find the best solution. {recipes}, {parsnip}, {tune}, {dials}, etc are separate packages used in a tidy machine learning development workflow. It may seem inconvenient to have so many packages to perform specific tasks, but such a paradigm is helpful for decomposing the whole model design process, often making problems feel more manageable. It is inclusive, because the tidyverse is not just the collection of packages, but it is also the community of people who use them. Although the {tidyverse} and {tidymodels} are opinionated in their design, the developers are receptive to public feedback. "],["meeting-videos-2.html", "3.5 Meeting Videos", " 3.5 Meeting Videos 3.5.1 Cohort 1 Meeting chat log 00:06:11 Jonathan Trattner: I second that 00:08:03 Tan Ho: Time to find and send memez 00:09:11 Tan Ho: You all have seen Hadley cat vibing, right? 00:09:18 Jon Harmon (jonthegeek): https://www.vivino.com/mcpherson-cellars-la-herencia/w/2270344?ref=nav-search&amp;cart_item_source=text-search 00:10:00 Jon Harmon (jonthegeek): https://twitter.com/RCoderWeb/status/1351282600086810634 00:10:18 David Severski: Any chance you could get a bit closer to your mic, Tony? Audio is a bit poor on my end. 00:10:43 Jon Harmon (jonthegeek): I muted the one that was crackling, I think. 00:10:50 Maya Gans: Friendly reminder to please mute :) 00:10:54 Maya Gans: Oh you beat me thanks 00:13:16 Jon Harmon (jonthegeek): New base pipe makes this all... weird. But they have a way to do it now, at least! 00:13:53 Jon Harmon (jonthegeek): trees |&gt; my_data =&gt; lm(Volume ~ ., data = my_data) 00:13:55 Jordan Krogmann: I haven&#39;t played around with the base pipe yet 00:14:17 Tan Ho: =&gt; ??? 00:14:24 Jordan Krogmann: that is going to take some re-learning 00:14:27 David Severski: Is there a comprehensive guide to R’s formula syntax? Always found learning I(), + , *, etc. kind of scattered through bits of documentation. 00:14:28 Jon Harmon (jonthegeek): Kinda a lambda function... thing. 00:14:31 Jordan Krogmann: &quot;=&gt;&quot;? 00:14:53 Maya Gans: I thought `I` was “asis” - is it different inside the context of lm? 00:14:56 Jon Harmon (jonthegeek): But don&#39;t get hung up on that &#39;cuz it&#39;s still in development, RStudio will make it clear when it&#39;s time :) 00:15:36 Jon Harmon (jonthegeek): @David: Hmm, I haven&#39;t seen a formula cheat sheet, but there HAS to be one out there... 00:17:35 Tyler Grant Smith: https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf page 57 00:18:12 Yoni Sidi: There is a Belgian flavored cheatsheet, but not going to share anything from said site 00:19:20 Jon Harmon (jonthegeek): Yeahhhh, no thanks! The Paradis book has a good guide in there, it&#39;d be nice to pull that out... so let&#39;s do that in the notes for this chapter! 00:20:45 David Severski: Shout out to gglm for better lm plots with ggplot! http://graysonwhite.com/gglm/ 00:21:02 luifrancgom: Also ggplot2::geom_qq(): https://ggplot2.tidyverse.org/reference/geom_qq.html 00:21:24 Tan Ho: Belgian...flavoured... 00:21:37 Tan Ho: ohhhhh 00:21:39 Jon Harmon (jonthegeek): Reference to a litigious website. 00:21:43 Tan Ho: I was thinking of waffles 00:21:58 tim: Now I&#39;m thinking about waffles 00:22:15 Yoni Sidi: +1 on waffles 00:22:30 Jon Harmon (jonthegeek): https://github.com/hrbrmstr/waffle 00:22:39 David Severski: Gotta be a minority here but pancakes &gt; waffles. ;) 00:22:55 Jon Harmon (jonthegeek): My wife&#39;s pancakes &gt; waffles. 00:23:07 Tan Ho: crepes? 00:23:10 Maya Gans: Insert {stacks} pun 00:23:20 Yoni Sidi: blinches&gt;pancakes 00:24:32 Conor Tompkins: What is the difference between modeling each subgroup separately vs. adding the subgroup as a term in a model that contains all the subgroups? 00:24:32 Yoni Sidi: Side note, you can pass arguments to the broom arguments on the res of the function in map 00:24:41 Yoni Sidi: *rhs 00:27:30 Andrew G. Farina: I think the biggest difference is robustness. modeling each subgroup lets you know how well the model fits that subgroup. Adding in a variable (subgroup) will tell you how much variance is accounted for by the subgroup. Does that make sense? 00:28:05 DX: Hi I am new to the book club. Where can we find this book? 00:28:24 Ben Gramza: https://www.tmwr.org/base-r.html 00:28:28 DX: Thanks 00:28:31 Scott Nestler: I used to get confused about &#39;anova&#39; and &#39;aov&#39; but they are very different. &#39;aov&#39; fits a model (by calling lm) whereas &#39;anova&#39; is a generic function that analyzes a fitted model (or models). 00:28:36 Joe Sydlowski: One reason I&#39;ve used subgroups instead of a categorical variable is when I want to do feature selection with the subgroups. If you use a categorical variable than each level is confined to the same predictors 00:29:00 Maya Gans: Thats a super useful distinction @Scott ! 00:31:03 Conor Tompkins: Andrew, Joe, thanks. I think I see what the benefit is. 00:31:51 Jon Harmon (jonthegeek): If someone wants to pipe up and summarize what Andrew and Joe explained, that&#39;d be great :D 00:32:22 luifrancgom: jajajaja 00:32:40 Jon Harmon (jonthegeek): btw data is also a function, so... be careful :) 00:33:08 Yoni Sidi: Last week me can confirm 00:33:15 Maya Gans: I love using single letters and t always gets me too 00:33:18 Scott Nestler: Many years ago, a common way to estimate a standard normal distribution was to sum 12 Uniform(0,1) distributions and then subtract 6. 00:33:19 luifrancgom: interesting from utils (data) 00:33:23 Jon Harmon (jonthegeek): For following along: r4ds.io/tmwr is this study guide... thing... that we&#39;re making 00:34:09 Jon Harmon (jonthegeek): And I use extremely specific variable names. RStudio autocomplete means you only have to type the full thing once, and then people know what you mean. 00:34:48 Tan Ho: trying to grok joe&#39;s comments for my non-statsy self: fitting one model with a categorical variable means you have one coefficient for each feature and the difference between categories is explained by the coefficient for the category. Fitting one model for each level of the categorical variable means you get different coefficients for a feature based on the subgroup of data 00:35:31 Yoni Sidi: It also depends if you incl the intercept 00:36:23 Yoni Sidi: w and wo it changes the meaning of the coefficients 00:36:48 Conor Tompkins: Sounds like subgrouping lets you turn non-tree models into a model with more tree-ish logic 00:38:02 Andrew G. Farina: I think that is a good way to think about it. If you had multiple models and were comparing fit. A linear model may fit one group better then the others, while a poly model may fit another group better. 00:38:12 luifrancgom: Thank you Tony 00:38:13 Jordan Krogmann: nice work! 00:38:16 Andrew G. Farina: Tony that was great 00:38:17 Scott Nestler: I learned a new term from reading this chapter. In the Advanced R book, we talked about R being &quot;lazy,&quot; but didn&#39;t know what the opposite of that was. Apparently, the word is &quot;eager.&quot; 00:38:40 Maya Gans: Thanks Tony!!! 00:38:40 darynr: Good job, Tony 00:38:48 Tan Ho: YAY TONY 00:38:50 Jonathan Trattner: thanks Tony! 00:39:01 Jonathan Leslie: Thanks, Tony! 00:39:01 Jim Gruman: thank you Tony 00:39:03 Conor Tompkins: Thanks Tony! 00:39:14 caroline: Thank you Tony 00:43:12 Tan Ho: &quot;There are houses&quot; /fin 00:43:40 Scott Nestler: There are actually a bunch of Ames housing data set memes out there. 00:44:21 Jonathan Trattner: https://gallery.shinyapps.io/ames-explorer/ 00:44:44 Jonathan Trattner: doesn&#39;t map stuff though 00:44:54 Jonathan Trattner: so leaflet is still an option! 00:45:32 David Severski: Gotta run here. Thanks everyone! 00:45:59 Jordan Krogmann: Thanks jon and tony later! "],["the-ames-housing-data.html", "Chapter 4 The Ames housing data", " Chapter 4 The Ames housing data Learning objectives: Explain why exploratory data analysis is an essential component of any modeling project. Recognize the Ames housing data - variables, context, and past cleaning. Explain when it makes sense to log-transform data. "],["pittsburgh-a-parallel-real-world-example.html", "4.1 Pittsburgh: a parallel real world example", " 4.1 Pittsburgh: a parallel real world example Conor Tompkins presented a fantastic overview of home sale price modeling by taking us through his recent project on Pittsburgh home sale price modeling, including discussions about his exploratory data analysis, motivations behind log-transforming sale data, and thoughts about inflation-adjusting historical sale prices. You can check out the discussion and presentation in the Cohort 1 meeting video for this week! Code Repository here: https://github.com/conorotompkins/model_allegheny_house_sales Shiny app: https://conorotompkins.shinyapps.io/house_sale_estimator/ "],["meeting-videos-3.html", "4.2 Meeting Videos", " 4.2 Meeting Videos 4.2.1 Cohort 1 Meeting chat log 00:11:41 Tyler Grant Smith: does jon sound far away 00:11:44 Jonathan Trattner: yes 00:11:58 Yoni Sidi: Austin is far away 00:12:04 Tan Ho: very! 00:12:23 Conor Tompkins: https://conorotompkins.shinyapps.io/house_sale_estimator/ 00:12:26 Jon Harmon (jonthegeek): I ran out of USB slots temporarily so I&#39;m using my crappy microphone for a bit. 00:26:41 Yoni Sidi: what were the original motivations on cleaning the data, was it preset task driven or strictly data driven? 00:27:59 Tan Ho: oops sorry :P 00:28:35 Juan Guillermo: Hi everyone 00:28:42 Jon Harmon (jonthegeek): Welcome, Juan! 00:28:57 Juan Guillermo: thanks! 00:28:59 Darya Vanichkina: With school districts vs council districts, how does that work? I.e. can school districts span counties, and does it make sense to adjust house price based on whether or not the house is in a good school district? [not US-based, so not 100% sure how it works on the ground] 00:29:46 Jon Harmon (jonthegeek): I don&#39;t know about Pittsburgh, but school districts &amp; council districts are completely unrelated in Austin. 00:29:46 Jonathan Trattner: Did you save and document each of those iterations in the data cleaning set? Or were you just going through it looking for what you wanted? 00:30:06 Tan Ho: YAY JON 00:30:12 Jonathan Trattner: Congrats!! 00:30:38 Jon Harmon (jonthegeek): Thanks, it&#39;s exciting! 00:37:56 Tony ElHabr: Conor, did you think about including additional data sets, such as Zillow&#39;s forecasts? 00:38:04 Asmae Toumi: skimr is awesome, it can generate all sorts of summaries. you can also pipe it after a group_by 00:38:19 Darya Vanichkina: It also works on the command line! 00:38:32 Darya Vanichkina: Which is really impressive when working on cloud/HPC 00:39:12 David Severski: Zillow is pretty tight fisted about scraping their estimates. 00:39:28 pavitra: is esquisse comparable with skimr? 00:40:30 pavitra: cool.. thanks! 00:40:43 Kevin Kent: Ohh esquisse is sketch in French. That makes sense 00:41:05 Jon Harmon (jonthegeek): And now French speakers can laugh at me, Pavitra, AND Yoni&#39;s pronunciation! 00:41:44 pavitra: well, I pronounced it like a total desi - &quot;eskqueeeeez&quot;..you cannot top that, Jon! 00:42:11 Asmae Toumi: Really happy I tuned it tonight, didn’t know of priceR package to inflation adjust prices. Ive been doing it manually lol 00:42:24 Jon Harmon (jonthegeek): yeah, that&#39;s great even on its own for sure! 00:42:45 Jon Harmon (jonthegeek): He said GitHub so we need to make him give us the URL so we can put it in the book. 00:42:48 Tan Ho: I&#39;ve been making use of CANSIM to access stats Canada data on stuff, i&#39;m sure there&#39;s something comparable 00:43:03 pavitra: does this dataset include demographics also? 00:43:06 Tan Ho: https://github.com/stevecondylios/priceR 00:43:27 Joe Sydlowski: I feel like I put a lot of blind trust in packages like that. How much do you validate the functions when you find a new package? 00:43:31 Jon Harmon (jonthegeek): @pavitra I don&#39;t think this one did. 00:43:42 Asmae Toumi: Speaking of hockey bruins currently kicking Pittsburgh’s ass right now 00:43:55 Tan Ho: boston home prices kicking everyone&#39;s ass rn 00:44:00 Asmae Toumi: lmaooooo 00:44:07 Yoni Sidi: def check the code and the level of unit testing 00:44:39 Tyler Grant Smith: wouldnt neighborhod effect vary qith year of sale....gentrification etc 00:44:42 Jonathan Trattner: It is on CRAN for whatever that’s worth 00:44:49 Yoni Sidi: that&#39;s not worth much 00:44:54 Tan Ho: He&#39;s also in the R4DS slack channel 00:44:58 Yoni Sidi: on CRAN means they passed cmd check 00:45:05 Jonathan Trattner: Well yeah 00:45:09 Jonathan Trattner: But it also has some nice tests 00:45:12 Tony ElHabr: yoni is going to need to interview him before he approves of the package 00:45:15 David Severski: A little basic looking. https://github.com/stevecondylios/priceR/blob/master/R/adjust_for_inflation.R#L298-L321 00:45:22 Jonathan Trattner: Using api for world bank 00:46:08 David Severski: I tend to use indices direct from FRED for a lot of my own inflation conversion work. 00:46:31 Jon Harmon (jonthegeek): Yeah, it definitely depends how important exact numbers are to you. 00:46:50 Kevin Kent: I guess inflation would be a feature you’d have to forecast out if you wanted to get predictions for the future? But still noodling on that. 00:47:07 Tyler Grant Smith: it definitely is 00:47:29 Jon Harmon (jonthegeek): It looks like that package allowed for future inflation. 00:47:41 Jon Harmon (jonthegeek): (Conor commented out that part, but it showed in his code) 00:48:53 Kevin Kent: Oh nice. I feel like I run into that a lot in forecasting contexts - needing to be careful about the features and how uncertain they are in the future. 00:49:41 Scott Nestler: An aside since we have some sports fans in the group. Pine-Richland is where Phil Jurkovic, the former ND backup QB (who&#39;s now the starter at BC) is from. 00:49:54 Asmae Toumi: nice 00:52:29 Tyler Grant Smith: id definitely consider esp since a lot of pricing is done as $/sqft 00:52:38 David Severski: I wonder if lot sizes would discretize cleanly. Lot sizes tend to bin, right? 00:52:44 Tyler Grant Smith: did you engineer something like that 00:52:58 Jarad Jones: To go along with Jon’s question about log transforming…..how would you all decide to do that or not? 00:53:38 Scott Nestler: How did you collapse factor variables? With fct_lump_n() or something else? 00:53:46 David Severski: Jarad - Plotting out the distributions is something I try to do consistently, then look to transforms to get close-er to a normal distribution. 00:54:18 Tyler Grant Smith: not just log transform but box cox transforms more generally 00:54:22 Kevin Kent: I’d say it also depends on the assumptions of the model, and if they require normally distributed features. 00:54:22 Darya Vanichkina: Yes, like David - eyeball it :( 00:54:34 Tony ElHabr: yup. non negative is big use case 00:54:38 Asmae Toumi: In the words of the iconic Andrew German, “Log transform, kids. And don’t listen to people who tell you otherwise.” 00:54:45 Darya Vanichkina: Box Cox or Yeo Johnson 00:54:46 Asmae Toumi: link:https://statmodeling.stat.columbia.edu/2019/08/21/you-should-usually-log-transform-your-positive-data/ 00:54:56 Tyler Grant Smith: i would talk but i have lots of loud kids around me 00:55:05 David Severski: “And trust me about the sunscreen…” ;) 00:55:33 Asmae Toumi: Don’t forget the two finger rule for sunscreen also 00:55:37 Darya Vanichkina: Kevin, I think that because we could be comparing models which do/do not require normally distributed residuals I’d transform (and then compare) 00:55:42 Arjun Paudel: anytime you have a big tail 00:55:55 Tony ElHabr: also, if you&#39;re combining two predictions, I think log-transformed has good theoretical properties 00:56:13 Kevin Kent: Yeah that makes sense 00:56:47 Jarad Jones: That’s helpful, thanks! 00:56:52 Scott Nestler: That student was trying to maximize their leverage. 00:57:05 Jon Harmon (jonthegeek): They left and came back to the assignment but it&#39;s sometimes hard to see that. 00:57:13 Darya Vanichkina: It doesn’t - but you’re usually comparing the performance of the two 00:57:18 Darya Vanichkina: Right? 00:57:43 Tony ElHabr: yeah, I don&#39;t think it really needs it. but never hurts to try multiple methods 00:57:52 Tony ElHabr: tyler is the truth teller 00:58:00 Tony ElHabr: he got the kids to calm down for long enough 00:58:27 Tyler Grant Smith: yes that is one reason to do it 00:58:34 Tyler Grant Smith: theyre in the bath now 00:58:39 pavitra: for scientific assays, the dilutions are so large in range, I absolutely need to log-transform the data to make any sense of it 00:58:46 Jon Harmon (jonthegeek): ^^^ 00:59:00 Tony ElHabr: also, you look smarter if you log transform 00:59:04 Darya Vanichkina: LOL 00:59:07 Tony ElHabr: your audience will think you know what you&#39;re doing 00:59:10 shamsuddeen: lol 00:59:16 Kevin Kent: Lol fantastic 00:59:33 pavitra: john murdoch 00:59:33 Darya Vanichkina: I loved the RStudio conf talk where the FT journalist pros/cons of it 00:59:47 Darya Vanichkina: Yes, there are also “normal people”… 01:00:23 Tyler Grant Smith: lognormal people 01:01:35 Scott Nestler: 1 Full and 7 Half ??? 01:02:13 Jonathan Trattner: Maybe they’re complementary halves? 01:02:13 Tyler Grant Smith: shower in the bedroom but i cant be bothered to go to another room for #2 01:03:39 Tan Ho: doesn&#39;t your house have a three-urinal men&#39;s washroom separate from a three-stall women&#39;s washroom? 01:03:40 Darya Vanichkina: If possible, please, I’d love some documentation for all of the .R scripts on GitHub to make your thought process/prototyping clearer…. 01:03:43 Asmae Toumi: Is this on GitHub? I have a small aesthetic suggestion for the leaflet map so that the labels are on top of the color 01:04:15 Jon Harmon (jonthegeek): I believe it is and we&#39;ll make him share it in the channel/in the bookdown :) 01:04:17 Darya Vanichkina: I think it’s here? https://github.com/conorotompkins/model_allegheny_house_sales 01:04:28 Tan Ho: Yup, that&#39;s the one 01:04:43 Asmae Toumi: thanks 01:05:00 Darya Vanichkina: Sorry, need to run - thank you, everyone! 01:05:13 pavitra: has connor already developed any models on this data? 01:05:13 Jon Harmon (jonthegeek): See ya Darya! 01:05:26 Tan Ho: https://github.com/conorotompkins/model_allegheny_house_sales/tree/main/scripts/model @pavitra 01:05:28 Jon Harmon (jonthegeek): @pavitra: Yup! he predicts the price based on those settings. 01:05:47 pavitra: oh boy! neat 01:06:33 David Severski: Gotta run now. Thanks for the talk and the conversation! 01:07:30 Jon Harmon (jonthegeek): A numeric value between 0 and 1 or an integer greater or equal to one. If it&#39;s less than one then factor levels whose rate of occurrence in the training set are below threshold will be &quot;othered&quot;. If it&#39;s greater or equal to one then it&#39;s treated as a frequency and factor levels that occur less then threshold times will be &quot;othered&quot;. 01:09:13 Jon Harmon (jonthegeek): A logical. Should the step be skipped when the recipe is baked by bake.recipe()? While all operations are baked when prep.recipe() is run, some operations may not be able to be conducted on new data (e.g. processing the outcome variable(s)). Care should be taken when using skip = TRUE as it may affect the computations for subsequent operations 01:11:30 Jim Gruman: thankyou Conor 01:11:40 Kevin Kent: Nice job! Was helpful to talk through code and the concepts represented in it. 01:11:44 Tan Ho: thank you! that was awesome! 01:11:48 Jonathan Leslie: Thanks, Conor! 01:11:51 caroline: Thank you Conor 01:11:52 Joe Sydlowski: Thanks Conor! 01:11:56 Laurens Put: Thank you 01:11:59 Asmae Toumi: Conor that was awesome. I hope it ends up on tidytuesday 01:12:00 Jarad Jones: Nice job Conor, the whole end product is pretty impressive for a first shiny app! 01:12:00 pavitra: thanks a lot, Connor..i think you finished the purpose of the book "],["spending-our-data.html", "Chapter 5 Spending our data", " Chapter 5 Spending our data Learning objectives: Use {rsample} to split data into training and testing sets. Identify cases where stratified sampling is useful. Understand the difference between rsample::initial_time_split() and rsample::initial_split(). Understand the trade-offs between too little training data and too little testing data. Define a validation set of data. Explain why data should be split at the independent experimental unit level. "],["spending-our-data-1.html", "5.1 Spending our data", " 5.1 Spending our data The task of creating a useful model can be daunting. Thankfully, one can do so step-by-step. It can be helpful to sketch out your path, as Chanin Nantasenamat has done so: We’re going to zoom into the data splitting part. As the diagram shows, it is one of the earliest considerations in a model building workflow. The training set is the data that the model(s) learns from. It’s usually the majority of the data (~ 80-70% of the data), and you’ll be spending the bulk of your time working on fitting models to it. The test set is the data set aside for unbiased model validation once a candidate model(s) has been chosen. Unlike the training set, the test set is only looked at once. Why is it important to think about data splitting? You could do everything right, from cleaning the data, collecting features and picking a great model, but get bad results when you test the model on data it hasn’t seen before. If you’re in this predicament, the data splitting you’ve employed may be worth further investigation. "],["common-methods-for-splitting-data.html", "5.2 Common methods for splitting data", " 5.2 Common methods for splitting data Choosing how to conduct the split of the data into training and test sets may not be a trivial task. It depends on the data and the purpose. The most common type of sampling is known as random sampling and it is done readily in R using the rsample package with the initial_split()function. For the Ames housing dataset, the call would be: library(tidymodels) set.seed(123) data(ames) ames_split &lt;- initial_split(ames, prob = 0.80) ames_split ## &lt;Analysis/Assess/Total&gt; ## &lt;2198/732/2930&gt; The object ames_split is an rsplit object. To get the training and test results you can call on training() and test(): ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) "],["class-imbalance.html", "5.3 Class imbalance", " 5.3 Class imbalance In many instances, random splitting is not suitable. This includes datasets that contain class imbalance, meaning one class is dominated by another. Class imbalance is important to detect and take into consideration in data splitting. Performing random splitting on a dataset with severe class imbalance may cause the model to perform badly at validation. You want to avoid allocating the minority class disproportionately into the training or test set. The point is to have the same distribution across the training and test sets. Class imbalance can occur in differing degrees: Splitting methods suited for datasets containing class imbalance should be considered. Let’s consider a #Tidytuesday dataset on Himalayan expedition members, which Julia Silge recently explored here using {tidymodels}. library(tidyverse) library(skimr) members &lt;- read_csv(&quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/members.csv&quot;) skim(members) Table 5.1: Data summary Name members Number of rows 76519 Number of columns 21 _______________________ Column type frequency: character 10 logical 6 numeric 5 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace expedition_id 0 1.00 9 9 0 10350 0 member_id 0 1.00 12 12 0 76518 0 peak_id 0 1.00 4 4 0 391 0 peak_name 15 1.00 4 25 0 390 0 season 0 1.00 6 7 0 5 0 sex 2 1.00 1 1 0 2 0 citizenship 10 1.00 2 23 0 212 0 expedition_role 21 1.00 4 25 0 524 0 death_cause 75413 0.01 3 27 0 12 0 injury_type 74807 0.02 3 27 0 11 0 Variable type: logical skim_variable n_missing complete_rate mean count hired 0 1 0.21 FAL: 60788, TRU: 15731 success 0 1 0.38 FAL: 47320, TRU: 29199 solo 0 1 0.00 FAL: 76398, TRU: 121 oxygen_used 0 1 0.24 FAL: 58286, TRU: 18233 died 0 1 0.01 FAL: 75413, TRU: 1106 injured 0 1 0.02 FAL: 74806, TRU: 1713 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist year 0 1.00 2000.36 14.78 1905 1991 2004 2012 2019 ▁▁▁▃▇ age 3497 0.95 37.33 10.40 7 29 36 44 85 ▁▇▅▁▁ highpoint_metres 21833 0.71 7470.68 1040.06 3800 6700 7400 8400 8850 ▁▁▆▃▇ death_height_metres 75451 0.01 6592.85 1308.19 400 5800 6600 7550 8830 ▁▁▂▇▆ injury_height_metres 75510 0.01 7049.91 1214.24 400 6200 7100 8000 8880 ▁▁▂▇▇ Let’s say we were interested in predicting the likelihood of survival or death for an expedition member. It would be a good idea to check for class imbalance: library(janitor) members %&gt;% tabyl(died) %&gt;% adorn_totals(&quot;row&quot;) ## died n percent ## FALSE 75413 0.98554607 ## TRUE 1106 0.01445393 ## Total 76519 1.00000000 We can see that nearly 99% of people survive their expedition. This dataset would be ripe for a sampling technique adept at handling such extreme class imbalance. This technique is called stratified sampling, in which “the training/test split is conducted separately within each class and then these subsamples are combined into the overall training and test set”. Operationally, this is done by using the strata argument inside initial_split(): set.seed(123) members_split &lt;- initial_split(members, prob = 0.80, strata = died) members_train &lt;- training(members_split) members_test &lt;- testing(members_split) "],["continuous-outcome-data.html", "5.4 Continuous outcome data", " 5.4 Continuous outcome data For continuous outcome data (e.g. costs), a stratified random sampling approach would involve conducting a 80/20 split within each quartile and then pool the results together. For the Ames housing dataset, the call would look like this: set.seed(123) ames_split &lt;- initial_split(ames, prob = 0.80, strata = Sale_Price) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) "],["time-series-data.html", "5.5 Time series data", " 5.5 Time series data For time series data where you’d want to allocate data to the training set/test set depending on a sorted order, you can use initial_time_split() which works similarly to initial_split(). The prop argument can be used to specify what proportion of the first part of data should be used as the training set. data(drinks) drinks_split &lt;- initial_time_split(drinks) train_data &lt;- training(drinks_split) test_data &lt;- testing(drinks_split) The lag argument can specify a lag period to use between the training and test set. This is useful if lagged predictors will be used during training and testing. drinks_lag_split &lt;- initial_time_split(drinks, lag = 12) train_data_lag &lt;- training(drinks_lag_split) test_data_lag &lt;- testing(drinks_lag_split) c(max(train_data_lag$date), min(test_data_lag$date)) ## [1] &quot;2011-03-01&quot; &quot;2010-04-01&quot; "],["multi-level-data.html", "5.6 Multi-level data", " 5.6 Multi-level data It’s important to figure out what the independent experimental unit is in your data. In the Ames dataset, there is one row per house and so houses and their properties are considered to be independent of one another. In other datasets, there may be multiple rows per experimental unit (e.g. as in patients who are measured multiple times across time). This has implications for data splitting. To avoid data from the same experimental unit being in both the training and test set, split along the independent experimental units such that X% of experimental units are in the training set. "],["what-proportion-should-be-used.html", "5.7 What proportion should be used?", " 5.7 What proportion should be used? Where does the 80/20 training/testing split come from?&mdash; Asmae Toumi (@asmae_toumi) January 31, 2021 Some people said the 80/20 split comes from the Pareto principle/distribution or the power law. Some said because it works nicely with 5-fold cross-validation (which we will see in the later chapters). I believe the point is to use enough data in the training set to allow for solid parameter estimation but not too much that it hurts performance. 80/20 or 70/30 seems reasonable for most problems at hand, as it’s what is widely used. Max Kuhn notes that a test set is almost always a good idea, and it should only be avoided when the data is “pathologically small”. "],["summary.html", "5.8 Summary", " 5.8 Summary Data splitting is an important part of a modeling workflow as it impacts model validity and performance. The most common splitting technique is random splitting. Some data, such as time-series or multi-level data require a different data splitting technique called stratified sampling. The rsample package contains many functions that can perform random splitting and stratified splitting. We will learn more about how to remedy certain issues such as class imbalance, bias and overfitting in Chapter 10. 5.8.1 References Tidy modeling with R by Max Kuhn and Julia Silge: https://www.tmwr.org/splitting.html Feature Engineering and Selection: A Practical Approach for Predictive Models by Max Kuhn and Kjell Johnson: https://bookdown.org/max/FES/ Handle class imbalance in #TidyTuesday climbing expedition data with tidymodels: https://juliasilge.com/blog/himalayan-climbing/ Data preparation and feature engineering for machine learning: https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data How to Build a Machine Learning Model by Chanin Nantasenamat: https://towardsdatascience.com/how-to-build-a-machine-learning-model-439ab8fb3fb1 "],["feature-engineering-with-recipes.html", "Chapter 6 Feature engineering with recipes", " Chapter 6 Feature engineering with recipes Learning objectives: Define feature engineering. List reasons that feature engineering might be beneficial. Use the {recipes} package to create a simple feature engineering recipe. Use selectors from the {recipes} package to apply transformations to specific types of columns. List some advantages of using a recipe for feature engineering. Describe what happens when a recipe is prepared with recipes::prep(). Use recipes::bake() to process a dataset. Recognize how to use recipes::step_unknown(), recipes::step_novel(), recipes::step_other() to prepare factor variables. Explain how recipes::step_dummy() encodes qualitative data in a numeric format. Recognize techniques for dealing with large numbers of categories, such as feature hashing or encoding using the {embed} package (as described in this talk by Alan Feder at rstudio::global(2021)). Recognize methods for encoding ordered factors. Use recipes::step_interact() to add interaction terms to a recipe. Understand why some steps might only be applicable to training data. Recognize the functions from {recipes} and {themis} that are only applied to training data by default. Recognize that {recipes} includes functions for creating spline terms, such as step_ns(). Recognize that {recipes} includes functions for feature extraction, such as step_pca(). Use themis::step_downsample() to downsample data. Recognize other row-sampling steps from the {recipes} package. Use recipes::step_mutate() and recipes::step_mutate_at() for general {dplyr}-like transformations. Recall that the {textrecipes} package exists for text-specific feature-engineering steps. Understand that the functions of the {recipes} package use training data for all preprocessing and feature engineering steps to prevent leakage. Use {recipes} to prepare data for traditional modeling functions. Use tidy() to examine a recipe and its steps. Refer to columns with roles other than \"predictor\" or \"outcome\". "],["slide-1-title.html", "6.1 Slide 1 Title", " 6.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title.html", "6.2 Slide 2 Title", " 6.2 Slide 2 Title Put the content of your second slide here. "],["fitting-models-with-parsnip.html", "Chapter 7 Fitting models with parsnip", " Chapter 7 Fitting models with parsnip Learning objectives: Jon will prefill these. "],["slide-1-title-1.html", "7.1 Slide 1 Title", " 7.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-1.html", "7.2 Slide 2 Title", " 7.2 Slide 2 Title Put the content of your second slide here. "],["a-model-workflow.html", "Chapter 8 A model workflow", " Chapter 8 A model workflow Learning objectives: Jon will prefill these. "],["slide-1-title-2.html", "8.1 Slide 1 Title", " 8.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-2.html", "8.2 Slide 2 Title", " 8.2 Slide 2 Title Put the content of your second slide here. "],["judging-model-effectiveness.html", "Chapter 9 Judging model effectiveness", " Chapter 9 Judging model effectiveness Learning objectives: Jon will prefill these. "],["slide-1-title-3.html", "9.1 Slide 1 Title", " 9.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-3.html", "9.2 Slide 2 Title", " 9.2 Slide 2 Title Put the content of your second slide here. "],["review-of-chapters-4-9.html", "Review of chapters 4-9", " Review of chapters 4-9 Learning objectives: Jon will prefill these. "],["slide-1-title-4.html", "9.3 Slide 1 Title", " 9.3 Slide 1 Title Put the content of your slide here. "],["slide-2-title-4.html", "9.4 Slide 2 Title", " 9.4 Slide 2 Title Put the content of your second slide here. "],["resampling-for-evaluating-performance.html", "Chapter 10 Resampling for evaluating performance", " Chapter 10 Resampling for evaluating performance Learning objectives: Jon will prefill these. "],["slide-1-title-5.html", "10.1 Slide 1 Title", " 10.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-5.html", "10.2 Slide 2 Title", " 10.2 Slide 2 Title Put the content of your second slide here. "],["comparing-models-with-resampling.html", "Chapter 11 Comparing models with resampling", " Chapter 11 Comparing models with resampling Learning objectives: Jon will prefill these. "],["slide-1-title-6.html", "11.1 Slide 1 Title", " 11.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-6.html", "11.2 Slide 2 Title", " 11.2 Slide 2 Title Put the content of your second slide here. "],["model-tuning-and-the-dangers-of-overfitting.html", "Chapter 12 Model tuning and the dangers of overfitting", " Chapter 12 Model tuning and the dangers of overfitting Learning objectives: Jon will prefill these. "],["slide-1-title-7.html", "12.1 Slide 1 Title", " 12.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-7.html", "12.2 Slide 2 Title", " 12.2 Slide 2 Title Put the content of your second slide here. "],["grid-search.html", "Chapter 13 Grid search", " Chapter 13 Grid search Learning objectives: Jon will prefill these. "],["slide-1-title-8.html", "13.1 Slide 1 Title", " 13.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-8.html", "13.2 Slide 2 Title", " 13.2 Slide 2 Title Put the content of your second slide here. "],["iterative-search.html", "Chapter 14 Iterative search", " Chapter 14 Iterative search Learning objectives: Jon will prefill these. "],["slide-1-title-9.html", "14.1 Slide 1 Title", " 14.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-9.html", "14.2 Slide 2 Title", " 14.2 Slide 2 Title Put the content of your second slide here. "],["review-of-chapters-10-14.html", "Review of chapters 10-14", " Review of chapters 10-14 Learning objectives: Jon will prefill these. "],["slide-1-title-10.html", "14.3 Slide 1 Title", " 14.3 Slide 1 Title Put the content of your slide here. "],["slide-2-title-10.html", "14.4 Slide 2 Title", " 14.4 Slide 2 Title Put the content of your second slide here. "]]
