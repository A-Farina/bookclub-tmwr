[["index.html", "Tidy Modeling with R Book Club Welcome", " Tidy Modeling with R Book Club The R4DS Online Learning Community 2021-02-03 Welcome This is a companion for the book Tidy Modeling with R by Max Kuhn and Julia Silge. This companion is available at r4ds.io/tmwr. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book. This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["software-for-modeling.html", "Chapter 1 Software for modeling", " Chapter 1 Software for modeling Learning objectives: Recognize the principles around which the {tidymodels} packages were designed. Classify models as descriptive, inferential, and/or predictive. Define descriptive model. Define inferential model. Define predictive model. Differentiate between supervised and unsupervised models. Differentiate between regression and classification models. Differentiate between quantitative and qualitative data. Understand the roles that data can have in an analysis. Apply the data science process. Recognize the phases of modeling. The utility of a model hinges on its ability to be reductive. The primary influences in the data can be captured mathematically in a useful way, such as in a relationship that can be expressed as an equation. There are two reasons that models permeate our lives today: an abundance of software exists to create models and it has become easier to record data and make it accessible. "],["the-pit-of-success.html", "1.1 The pit of success", " 1.1 The pit of success {tidymodels} aims to help us fall into the Pit of Success: The Pit of Success: in stark contrast to a summit, a peak, or a journey across a desert to find victory through many trials and surprises, we want our customers to simply fall into winning practices by using our platform and frameworks. Avoid confusion: Software should facilitate proper usage. Avoid mistakes: Software should make it easy for users to do the right thing. "],["types-of-models.html", "1.2 Types of models", " 1.2 Types of models Descriptive models: Describe or illustrate characteristics of data. Inferential models: Make some statement of truth regarding a predefined conjecture or idea. Inferential techniques typically produce some type of probabilistic output, such as a p-value, confidence interval, or posterior probability. Usually delayed feedback between inference and actual result. Predictive models: Produce the most accurate possible prediction for new data. Estimation (“How much?”) rather than inference (“Will it?”). Mechanistic models are derived using first principles to produce a model equation that is dependent on assumptions. Depend on the assumptions that define their model equations. Unlike inferential models, it is easy to make data-driven statements about how well the model performs based on how well it predicts the existing data Empirically driven models have more vague assumptions, and are derived directly from the data. No theoretical or probabilistic assumptions are made about the equations or the variables The primary method of evaluating the appropriateness of the model is to assess its accuracy using existing data 1. Broader discussions of these distinctions can be found in Breiman (2001b) and Shmueli (2010) "],["terminology.html", "1.3 Terminology", " 1.3 Terminology Unsupervised models are used to understand relationships between variables or sets of variables without an explicit relationship between variables and an outcome. Examples: PCA, clustering, autoencoders. Supervised models have an outcome variable. Examples: linear regression, neural networks. Regression: numeric outcome Classification: ordered or unordered qualitative values. Quantitative data: numbers. Qualitative (nominal) data: non-numbers. Data can have different roles in analyses: Outcomes (labels, endpoints, dependent variables): the value being predicted in supervised models. Predictors (independent variables): the variables used to predict the outcome. "],["the-data-analysis-process.html", "1.4 The data analysis process", " 1.4 The data analysis process Cleaning the data: investigate the data to make sure that they are applicable to the project goals, accurate, and appropriate Understanding the data: often referred to as exploratory data analysis (EDA). EDA brings to light how the different variables are related to one another, their distributions, typical ranges, and other attributes. “How did I come by these data?” “Is the data relevant?” Develop clear expectations of the goal of your model and how performance will be judged (Chapter 9) “What is/are the performance metrics or realistic goal/s of what can be achieved?” The data science process (from R for Data Science by Wickham and Grolemund. "],["the-modeling-process.html", "1.5 The modeling process", " 1.5 The modeling process The modeling process. Exploratory data analysis: Explore the data to see what they might tell you. (See previous) Feature engineering: Create specific model terms that make it easier to accurately model the observed data. Covered in Chapter 6. Model tuning and selection: Generate a variety of models and compare performance. Some models require hyperparameter tuning Model evaluation: Use EDA-like analyses and compare model performance metrics to choose the best model for your situation. "],["a-tidyverse-primer.html", "Chapter 2 A tidyverse primer", " Chapter 2 A tidyverse primer Learning objectives: List the tidyverse design principles. Explain what it means for the tidyverse to be designed for humans. Describe how reusing existing data structures can make functions easier to work with. Explain what it means for a set of functions to be designed for the pipe. Explain what it means for function to be designed for functional programming. List some differences between a tibble and a base data.frame. Recognize how to use the tidyverse to read and wrangle data. "],["tidyverse-design-principles.html", "2.1 Tidyverse design Principles", " 2.1 Tidyverse design Principles The tidyverse has four core design principles: Human centered: Designed to promote human usability. Consistent: Learning how to use one function or package is as similar as another. Composable: Easily breakdown data challenges into smaller components with exploratory tools to find the best solution. Inclusive: Fostering a community of like-minded users (e.g. #rstats) "],["design-for-humans.html", "2.2 Design for Humans", " 2.2 Design for Humans “Programs must be written for people to read, and only incidentally for machines to execute.” - Hal Abelson The tidyverse offers packages that are easily readable and understood by humans. It enables them to more easily achieve their programming goals. Consider the mtcars dataset, which comprises fuel consumption and 10 aspects of autombile design and performance from 1973-1974. Previewing the first six rows of the data, we see: ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 If we wanted to arrange these in ascending order based on the mpg and gear variables, how could we do this? The function arrange(), in the dplyr package of the tidyverse, takes a data frame and column names as such: arrange(.data = mtcars, gear, mpg) arrange(), and other tidyverse functions, use names that are descriptive and explicit. For general methods, there is a focus on verbs, as seen with the functions pivot_longer() and pivot_wider() in the tidyr package. "],["reusing-existing-data-structures.html", "2.3 Reusing existing data structures", " 2.3 Reusing existing data structures “You don’t have to reinvent the wheel, just attach it to a new wagon.” - Mark McCormack There are many different data types in R, such as matrices, lists, and data frames.1 A typical function would take in data of some form, conduct an operation, and return the result. tidyverse functions most often operate on data structures called tibbles. Traditional data frames can represent different data types in each column, and multiple values in each row. Tibbles are a special data frame that have additional properties helpful for data analysis. Example: list-columns boot_samp &lt;- rsample::bootstraps(mtcars, times = 3) boot_samp ## # Bootstrap sampling ## [90m# A tibble: 3 x 2[39m ## splits id ## [3m[90m&lt;list&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m ## [90m1[39m [90m&lt;split [32/15]&gt;[39m Bootstrap1 ## [90m2[39m [90m&lt;split [32/15]&gt;[39m Bootstrap2 ## [90m3[39m [90m&lt;split [32/8]&gt;[39m Bootstrap3 class(boot_samp) ## [1] &quot;bootstraps&quot; &quot;rset&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; The above example shows how to create bootstrap resamples of the data frame mtcars. It returns a tibble with a splits column that defines the resampled data sets. This function inherits data frame and tibble methods so other functions that operate on those data structures can be used. For a more detailed discussion, see Hadley Wickham’s Advanced R↩︎ "],["designed-for-the-pipe.html", "2.4 Designed for the pipe", " 2.4 Designed for the pipe The pipe operator, %&gt;%, comes from the magrittr package by Stefan Milton Bache, and is used to chain together a sequence of R functions. More specifically, the pipe operator uses the value of the object on the left-hand side of the operator as the first argument on the operator’s right-hand side. The pipe allows for highly readable code. Consider wanting to sort the mtcars dataset by the number of gears (gear) and then select the first ten rows. How would you do that? cars_arranged &lt;- arrange(mtcars, gear) cars_selected &lt;- slice(cars_arranged, 1:10) # more compactly cars_selected &lt;- slice(arrange(mtcars, gear), 1:10) Using the pipe to substitute the left-hand side of the operator with the first argument on the right-hand side, we can get the same result as follows: cars_selected &lt;- mtcars %&gt;% arrange(gear) %&gt;% slice(1:10) This approach with the pipe works because all the functions return the same data structure (a tibble/data frame) which is the first argument of the next function. Whenever possible, create functions that can be incorporated into a pipeline of operations. "],["designed-for-functional-programming.html", "2.5 Designed for Functional Programming", " 2.5 Designed for Functional Programming Functional Programming is an approach to replace iterative (i.e. for) loops. Consider the case where you may want two times the square root of the mpg for each car in mtcars. You could do this with a for loop as follows: n &lt;- nrow(mtcars) roots &lt;- rep(NA_real_, n) for (car in 1:n) { roots[car] &lt;- 2 * sqrt(mtcars$mpg[car]) } You could also write a function to do the computations. In functional programming, it’s important that the function does not have any side effects and the output only depends on the inputs. For example, the function my_sqrt() takes in a car’s mpg and a weight by which to multiply the square root. my_sqrt &lt;- function(mpg, weight) { weight * sqrt(mpg) } Using the purrr package, we can forgo the for loop and use the map() family of functions which use the basic syntax of map(vector, function). Below, we are applying the my_sqrt() function, with a weight of 2, to the first three elements of mtcars$mpg. User supplied functions can be declared by prefacing it with ~ (pronounced “twiddle”). By default, map() returns a list. If you know the class of a function’s output, you can use special suffixes. A character output, for example, would used by map_chr(), a double by map_dbl(), and a logical by map_lgl(). map( .x = head(mtcars$mpg, 3), ~ my_sqrt( mpg = .x, weight = 2 ) ) ## [[1]] ## [1] 9.165151 ## ## [[2]] ## [1] 9.165151 ## ## [[3]] ## [1] 9.549869 map() functions can be used with 2 inputs, by specifying map2() Requires arguments .x and .y map2( .x = head(mtcars$mpg, 3), .y = c(1,2,3), ~ my_sqrt( mpg = .x, weight = .y ) ) ## [[1]] ## [1] 4.582576 ## ## [[2]] ## [1] 9.165151 ## ## [[3]] ## [1] 14.3248 "],["tibbles-vs-data-frames.html", "2.6 Tibbles vs. Data Frames", " 2.6 Tibbles vs. Data Frames A tibble is a special type of data frame with some additional properties. Specifically: Tibbles work with column names that are not syntactically valid variable names. data.frame(`this does not work` = 1:2, oops = 3:4) ## this.does.not.work oops ## 1 1 3 ## 2 2 4 tibble(`this does work, though` = 1:2, `woohoo!` = 3:4) ## [90m# A tibble: 2 x 2[39m ## `this does work, though` `woohoo!` ## [3m[90m&lt;int&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m ## [90m1[39m 1 3 ## [90m2[39m 2 4 Tibbles prevent partial matching of arguments to avoid accidental errors df &lt;- data.frame(partial = 1:5) tbbl &lt;- tibble(partial = 1:5) df$part ## [1] 1 2 3 4 5 tbbl$part ## Warning: Unknown or uninitialised column: `part`. ## NULL Tibbles prevent dimension dropping, so subsetting data into a single column will never return a vector. df[, &quot;partial&quot;] ## [1] 1 2 3 4 5 tbbl[, &quot;partial&quot;] ## [90m# A tibble: 5 x 1[39m ## partial ## [3m[90m&lt;int&gt;[39m[23m ## [90m1[39m 1 ## [90m2[39m 2 ## [90m3[39m 3 ## [90m4[39m 4 ## [90m5[39m 5 Tibbles allow for list-columns, which can be a powerful tool when working with the purrr package. template_list &lt;- list(a = 1, b = 2, c = 3, d = 4, e = 5) data.frame(col = 1:5, list_col = template_list) ## col list_col.a list_col.b list_col.c list_col.d list_col.e ## 1 1 1 2 3 4 5 ## 2 2 1 2 3 4 5 ## 3 3 1 2 3 4 5 ## 4 4 1 2 3 4 5 ## 5 5 1 2 3 4 5 tibble(col = 1:5, list_col = template_list) ## [90m# A tibble: 5 x 2[39m ## col list_col ## [3m[90m&lt;int&gt;[39m[23m [3m[90m&lt;named list&gt;[39m[23m ## [90m1[39m 1 [90m&lt;dbl [1]&gt;[39m ## [90m2[39m 2 [90m&lt;dbl [1]&gt;[39m ## [90m3[39m 3 [90m&lt;dbl [1]&gt;[39m ## [90m4[39m 4 [90m&lt;dbl [1]&gt;[39m ## [90m5[39m 5 [90m&lt;dbl [1]&gt;[39m "],["how-to-read-and-wrangle-data.html", "2.7 How to read and wrangle data", " 2.7 How to read and wrangle data The following example shows how to use the tidyverse to read in data (with the readr package) and easily manipulate it (using the dplyr and lubridate packages). We will walk through these steps during our meeting. library(tidyverse) library(lubridate) url &lt;- &quot;http://bit.ly/raw-train-data-csv&quot; all_stations &lt;- # Step 1: Read in the data. readr::read_csv(url) %&gt;% # Step 2: filter columns and rename stationname dplyr::select(station = stationname, date, rides) %&gt;% # Step 3: Convert the character date field to a date encoding. # Also, put the data in units of 1K rides dplyr::mutate(date = lubridate::mdy(date), rides = rides / 1000) %&gt;% # Step 4: Summarize the multiple records using the maximum. dplyr::group_by(date, station) %&gt;% dplyr::summarize(rides = max(rides), .groups = &quot;drop&quot;) head(all_stations, 10) ## [90m# A tibble: 10 x 3[39m ## date station rides ## [3m[90m&lt;date&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m 2001-01-01 18th 0 ## [90m 2[39m 2001-01-01 35-Bronzeville-IIT 0.448 ## [90m 3[39m 2001-01-01 35th/Archer 0.318 ## [90m 4[39m 2001-01-01 43rd 0.211 ## [90m 5[39m 2001-01-01 47th-Dan Ryan 0.787 ## [90m 6[39m 2001-01-01 47th-South Elevated 0.427 ## [90m 7[39m 2001-01-01 51st 0.364 ## [90m 8[39m 2001-01-01 54th/Cermak 0 ## [90m 9[39m 2001-01-01 63rd-Dan Ryan 1.37 ## [90m10[39m 2001-01-01 69th 2.37 “This pipeline of operations illustrates why the tidyverse is popular. A series of data manipulations is used that have simple and easy to understand user interfaces; the series is bundled together in a streamlined and readable way. The focus is on how the user interacts with the software. This approach enables more people to learn R and achieve their analysis goals, and adopting these same principles for modeling in R has the same benefits.” - Max Kuhn and Julia Silge in Tidy Modeling with R "],["a-review-of-r-modeling-fundamentals.html", "Chapter 3 A review of R modeling fundamentals", " Chapter 3 A review of R modeling fundamentals Learning objectives: Specify model terms using the R formula syntax. List conveniences for modeling that are supported by the R formula syntax. Use anova() to compare models. Use summary() to inspect a model. Use predict() to generate new predictions from a model. List the three purposes that the R model formula serves. Recognize how the design for humans rubric is applied to {tidymodels} packages. Use broom::tidy() to standardize the structure of R objects. Use the {tidyverse} along with base modeling functions like lm() to produce multiple models at once. "],["r-formula-syntax.html", "3.1 R formula syntax", " 3.1 R formula syntax We’ll use the trees data set provided in {modeldata} (loaded with {tidymodels}) for demonstration purposes. Tree girth (in inches), height (in feet), and volume (in cubic feet) are provided. (Girth is somewhat like a measure of diameter.) library(tidyverse) library(tidymodels) theme_set(theme_minimal(base_size = 14)) data(trees) trees &lt;- as_tibble(trees) trees ## [90m# A tibble: 31 x 3[39m ## Girth Height Volume ## [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m 8.3 70 10.3 ## [90m 2[39m 8.6 65 10.3 ## [90m 3[39m 8.8 63 10.2 ## [90m 4[39m 10.5 72 16.4 ## [90m 5[39m 10.7 81 18.8 ## [90m 6[39m 10.8 83 19.7 ## [90m 7[39m 11 66 15.6 ## [90m 8[39m 11 75 18.2 ## [90m 9[39m 11.1 80 22.6 ## [90m10[39m 11.2 75 19.9 ## [90m# … with 21 more rows[39m Note that there is an analytical way to calculate tree volume from measures of diameter and height. We observe that Girth is strongly correlated with Volume trees %&gt;% corrr::correlate() ## [90m# A tibble: 3 x 4[39m ## term Girth Height Volume ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m Girth [31mNA[39m 0.519 0.967 ## [90m2[39m Height 0.519 [31mNA[39m 0.598 ## [90m3[39m Volume 0.967 0.598 [31mNA[39m Shame on you 😉 if you didn’t guess I would make a scatter plot given a data set with two variables. trees %&gt;% ggplot(aes(x = Girth, y = Height)) + geom_point(aes(size = Volume)) We can fit a linear regression model to predict Volume as a function of the other two features, using the formula syntax to save us from some typing. reg_fit &lt;- lm(Volume ~ ., data = trees) reg_fit ## ## Call: ## lm(formula = Volume ~ ., data = trees) ## ## Coefficients: ## (Intercept) Girth Height ## -57.9877 4.7082 0.3393 How would you write this without the formula syntax? If we want to get fancy with our pipes (%&gt;%), then we should wrap our formula with formula() trees %&gt;% lm(formula(Volume ~ .), data = .) ## ## Call: ## lm(formula = formula(Volume ~ .), data = .) ## ## Coefficients: ## (Intercept) Girth Height ## -57.9877 4.7082 0.3393 Interaction terms are easy to generate. inter_fit &lt;- lm(Volume ~ Girth * Height, data = trees) inter_fit ## ## Call: ## lm(formula = Volume ~ Girth * Height, data = trees) ## ## Coefficients: ## (Intercept) Girth Height Girth:Height ## 69.3963 -5.8558 -1.2971 0.1347 Same goes for polynomial terms. poly_fit &lt;- lm(Volume ~ Girth + I(Girth^2) + Height, data = trees) poly_fit ## ## Call: ## lm(formula = Volume ~ Girth + I(Girth^2) + Height, data = trees) ## ## Coefficients: ## (Intercept) Girth I(Girth^2) Height ## -9.9204 -2.8851 0.2686 0.3764 Excluding columns is intuitive. no_height_fit &lt;- lm(Volume ~ . - Height, data = trees) no_height_fit ## ## Call: ## lm(formula = Volume ~ . - Height, data = trees) ## ## Coefficients: ## (Intercept) Girth ## -36.943 5.066 The intercept term can be removed conveniently. no_intercept_fit &lt;- lm(Volume ~ . + 0, data = trees) no_intercept_fit ## ## Call: ## lm(formula = Volume ~ . + 0, data = trees) ## ## Coefficients: ## Girth Height ## 5.0440 -0.4773 To illustrate another convenience provided by formulas, let’s add a categorical column. trees2 &lt;- trees set.seed(42) trees2$group = sample(toupper(letters[1:4]), size = nrow(trees2), replace = TRUE) trees2 ## [90m# A tibble: 31 x 4[39m ## Girth Height Volume group ## [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m ## [90m 1[39m 8.3 70 10.3 A ## [90m 2[39m 8.6 65 10.3 A ## [90m 3[39m 8.8 63 10.2 A ## [90m 4[39m 10.5 72 16.4 A ## [90m 5[39m 10.7 81 18.8 B ## [90m 6[39m 10.8 83 19.7 D ## [90m 7[39m 11 66 15.6 B ## [90m 8[39m 11 75 18.2 B ## [90m 9[39m 11.1 80 22.6 A ## [90m10[39m 11.2 75 19.9 D ## [90m# … with 21 more rows[39m Encoding the categories as separate features is done auto-magically with the formula syntax. dummy_fit &lt;- lm(Volume ~ ., data = trees2) dummy_fit ## ## Call: ## lm(formula = Volume ~ ., data = trees2) ## ## Coefficients: ## (Intercept) Girth Height groupB groupC groupD ## -55.2921 4.6932 0.3093 -1.8367 -0.0497 0.6462 Under the hood, this is done by model.matrix(). model.matrix(Volume ~ ., data = trees2) %&gt;% head(10) ## (Intercept) Girth Height groupB groupC groupD ## 1 1 8.3 70 0 0 0 ## 2 1 8.6 65 0 0 0 ## 3 1 8.8 63 0 0 0 ## 4 1 10.5 72 0 0 0 ## 5 1 10.7 81 1 0 0 ## 6 1 10.8 83 0 0 1 ## 7 1 11.0 66 1 0 0 ## 8 1 11.0 75 1 0 0 ## 9 1 11.1 80 0 0 0 ## 10 1 11.2 75 0 0 1 3.1.1 Recap Purposes of R model formula: The formula defines the columns that are used by the model. The standard R machinery uses the formula to encode the columns into an appropriate format. The roles of the columns are defined by the formula. "],["inspecting-and-developing-models.html", "3.2 Inspecting and developing models", " 3.2 Inspecting and developing models Being the sound analysts that we are, we should check if the assumptions of linear regression are violated. The plot() generic function has a specific method for lm objects that generates various diagnostic plots. par(mfrow = c(1, 2)) plot(reg_fit, which = c(1, 2)) The second plot does not show any strong violation of the normality assumption. However, the first plot shows a violation of the linearity assumption (that there is a linear relationship between the response variable and the predictors). If the assumption were satisfied, the smooth red line would be like a straight horizontal line at y=0. Note that there is a {ggplot2} way to generate the same plots. library(ggfortify) autoplot(reg_fit, which = c(1, 2)) But what about the coefficients? summary(reg_fit) ## ## Call: ## lm(formula = Volume ~ ., data = trees) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.4065 -2.6493 -0.2876 2.2003 8.4847 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -57.9877 8.6382 -6.713 2.75e-07 *** ## Girth 4.7082 0.2643 17.816 &lt; 2e-16 *** ## Height 0.3393 0.1302 2.607 0.0145 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.882 on 28 degrees of freedom ## Multiple R-squared: 0.948, Adjusted R-squared: 0.9442 ## F-statistic: 255 on 2 and 28 DF, p-value: &lt; 2.2e-16 Use {broom} for a tidy version. library(broom) reg_fit %&gt;% tidy() ## [90m# A tibble: 3 x 5[39m ## term estimate std.error statistic p.value ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m (Intercept) -[31m58[39m[31m.[39m[31m0[39m 8.64 -[31m6[39m[31m.[39m[31m71[39m 2.75[90me[39m[31m- 7[39m ## [90m2[39m Girth 4.71 0.264 17.8 8.22[90me[39m[31m-17[39m ## [90m3[39m Height 0.339 0.130 2.61 1.45[90me[39m[31m- 2[39m reg_fit %&gt;% glance() %&gt;% glimpse() ## Rows: 1 ## Columns: 12 ## $ r.squared [3m[90m&lt;dbl&gt;[39m[23m 0.94795 ## $ adj.r.squared [3m[90m&lt;dbl&gt;[39m[23m 0.9442322 ## $ sigma [3m[90m&lt;dbl&gt;[39m[23m 3.881832 ## $ statistic [3m[90m&lt;dbl&gt;[39m[23m 254.9723 ## $ p.value [3m[90m&lt;dbl&gt;[39m[23m 1.071238e-18 ## $ df [3m[90m&lt;dbl&gt;[39m[23m 2 ## $ logLik [3m[90m&lt;dbl&gt;[39m[23m -84.45499 ## $ AIC [3m[90m&lt;dbl&gt;[39m[23m 176.91 ## $ BIC [3m[90m&lt;dbl&gt;[39m[23m 182.6459 ## $ deviance [3m[90m&lt;dbl&gt;[39m[23m 421.9214 ## $ df.residual [3m[90m&lt;int&gt;[39m[23m 28 ## $ nobs [3m[90m&lt;int&gt;[39m[23m 31 {purrr} and {dplyr} can help you scale up your modeling process. We can compare all of the models we made before. list( &#39;reg&#39; = reg_fit, &#39;inter&#39; = inter_fit, &#39;poly&#39; = poly_fit, &#39;no_height&#39; = no_height_fit, &#39;no_intercept&#39; = no_intercept_fit ) %&gt;% map_dfr(glance, .id = &#39;id&#39;) %&gt;% select(id, adj.r.squared) %&gt;% arrange(desc(adj.r.squared)) ## [90m# A tibble: 5 x 2[39m ## id adj.r.squared ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m poly 0.975 ## [90m2[39m inter 0.973 ## [90m3[39m no_intercept 0.968 ## [90m4[39m reg 0.944 ## [90m5[39m no_height 0.933 We observe that the polynomial fit is the best. We can create models for each group in trees2. reg_fits &lt;- trees2 %&gt;% group_nest(group) %&gt;% mutate( fit = map(data, ~ lm(formula(Volume ~ .), data = .x)), tidied = map(fit, tidy), glanced = map(fit, glance), augmented = map(fit, augment) ) .select_unnest &lt;- function(data, ...) { data %&gt;% select(group, ...) %&gt;% unnest(...) } reg_fits %&gt;% .select_unnest(tidied) ## [90m# A tibble: 12 x 6[39m ## group term estimate std.error statistic p.value ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m A (Intercept) -[31m44[39m[31m.[39m[31m6[39m 17.5 -[31m2[39m[31m.[39m[31m55[39m 0.031[4m2[24m ## [90m 2[39m A Girth 4.21 0.477 8.83 0.000[4m0[24m[4m0[24m[4m9[24m98 ## [90m 3[39m A Height 0.250 0.294 0.849 0.418 ## [90m 4[39m B (Intercept) -[31m66[39m[31m.[39m[31m1[39m 13.9 -[31m4[39m[31m.[39m[31m74[39m 0.017[4m8[24m ## [90m 5[39m B Girth 4.16 0.704 5.91 0.009[4m6[24m[4m9[24m ## [90m 6[39m B Height 0.520 0.123 4.24 0.024[4m0[24m ## [90m 7[39m C (Intercept) -[31m86[39m[31m.[39m[31m4[39m 90.5 -[31m0[39m[31m.[39m[31m954[39m 0.410 ## [90m 8[39m C Girth 4.83 0.747 6.47 0.007[4m4[24m[4m8[24m ## [90m 9[39m C Height 0.680 1.20 0.567 0.611 ## [90m10[39m D (Intercept) -[31m46[39m[31m.[39m[31m3[39m 14.8 -[31m3[39m[31m.[39m[31m14[39m 0.034[4m9[24m ## [90m11[39m D Girth 6.03 0.372 16.2 0.000[4m0[24m[4m8[24m[4m5[24m2 ## [90m12[39m D Height -[31m0[39m[31m.[39m[31m0[39m[31m26[4m8[24m[39m 0.214 -[31m0[39m[31m.[39m[31m125[39m 0.906 reg_fits %&gt;% .select_unnest(glanced) ## [90m# A tibble: 4 x 13[39m ## group r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m A 0.960 0.951 3.81 107. 5.27[90me[39m[31m-7[39m 2 -[31m31[39m[31m.[39m[31m3[39m 70.7 72.6 ## [90m2[39m B 0.935 0.891 2.20 21.5 1.66[90me[39m[31m-2[39m 2 -[31m11[39m[31m.[39m[31m2[39m 30.3 29.5 ## [90m3[39m C 0.946 0.910 4.06 26.2 1.26[90me[39m[31m-2[39m 2 -[31m14[39m[31m.[39m[31m8[39m 37.7 36.8 ## [90m4[39m D 0.990 0.985 2.80 194. 1.04[90me[39m[31m-4[39m 2 -[31m15[39m[31m.[39m[31m2[39m 38.4 38.2 ## [90m# … with 3 more variables: deviance [3m[90m&lt;dbl&gt;[90m[23m, df.residual [3m[90m&lt;int&gt;[90m[23m, nobs [3m[90m&lt;int&gt;[90m[23m[39m reg_fits %&gt;% .select_unnest(augmented) ## [90m# A tibble: 31 x 10[39m ## group Volume Girth Height .fitted .resid .hat .sigma .cooksd .std.resid ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m A 10.3 8.3 70 7.81 2.49 0.252 3.91 0.064[4m5[24m 0.758 ## [90m 2[39m A 10.3 8.6 65 7.82 2.48 0.283 3.90 0.078[4m0[24m 0.769 ## [90m 3[39m A 10.2 8.8 63 8.17 2.03 0.395 3.93 0.103 0.687 ## [90m 4[39m A 16.4 10.5 72 17.6 -[31m1[39m[31m.[39m[31m18[39m 0.134 4.01 0.005[4m6[24m[4m6[24m -[31m0[39m[31m.[39m[31m332[39m ## [90m 5[39m A 22.6 11.1 80 22.1 0.500 0.534 4.03 0.014[4m2[24m 0.192 ## [90m 6[39m A 19.1 12 75 24.6 -[31m5[39m[31m.[39m[31m54[39m 0.123 3.45 0.113 -[31m1[39m[31m.[39m[31m56[39m ## [90m 7[39m A 22.2 12.9 74 28.2 -[31m5[39m[31m.[39m[31m99[39m 0.083[4m7[24m 3.38 0.082[4m3[24m -[31m1[39m[31m.[39m[31m64[39m ## [90m 8[39m A 36.3 14.5 74 34.9 1.37 0.116 4.00 0.006[4m4[24m[4m3[24m 0.383 ## [90m 9[39m A 38.3 16 72 40.8 -[31m2[39m[31m.[39m[31m45[39m 0.330 3.90 0.101 -[31m0[39m[31m.[39m[31m787[39m ## [90m10[39m A 55.7 17.5 82 49.6 6.13 0.255 3.16 0.397 1.87 ## [90m# … with 21 more rows[39m "],["more-of-base-and-stats.html", "3.3 More of {base} and {stats}", " 3.3 More of {base} and {stats} R’s {base} and {stats} libraries have lots of built-in functions that help perform statistical analysis. For example, anova() can be used to compare two regression models quickly. anova(reg_fit, poly_fit) ## Analysis of Variance Table ## ## Model 1: Volume ~ Girth + Height ## Model 2: Volume ~ Girth + I(Girth^2) + Height ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 28 421.92 ## 2 27 186.01 1 235.91 34.243 3.13e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We observe that the second order term for Girth does indeed provide significant explanatory power to the model. (Formally, we reject the null hypothesis that the second order term for Girth is zero.) What is ANOVA? Use base R statistical function when someone tries to test your statistics knowledge. Question: If \\(U_1\\) and \\(U_2\\) are i.i.d. \\(Unif(0,1)\\) random variables, what is the distribution of \\(U_1 + U_2\\)? set.seed(42) n &lt;- 10000 u_1 &lt;- runif(n) u_2 &lt;- runif(n) .hist &lt;- function(x, ...) { hist(x, probability = TRUE,...) lines(density(x), col = &quot;blue&quot;, lwd = 2, ...) } layout(matrix(c(1,2,3,3), 2, 2, byrow = TRUE)) .hist(u_1) .hist(u_2) .hist(u_1 + u_2) Answer: Evidently it’s triangular. There are probably lots of functions that you didn’t know you even needed. add_column &lt;- function(data) { # Whoops! `df` should be `data` df %&gt;% mutate(dummy = 1) } trees %&gt;% add_column() ## Error in UseMethod(&quot;mutate&quot;): no applicable method for &#39;mutate&#39; applied to an object of class &quot;function&quot; df() is the density function for the F distribution with df1 and df2 degrees of freedom df ## function (x, df1, df2, ncp, log = FALSE) ## { ## if (missing(ncp)) ## .Call(C_df, x, df1, df2, log) ## else .Call(C_dnf, x, df1, df2, ncp, log) ## } ## &lt;bytecode: 0x7fb28b171628&gt; ## &lt;environment: namespace:stats&gt; "],["why-tidy-principles-and-tidymodels.html", "3.4 Why Tidy Principles and {tidymodels}?", " 3.4 Why Tidy Principles and {tidymodels}? The {tidyverse} has four guiding principles which {tidymodels} shares. It is human centered, i.e. the {tidyverse} is designed specifically to support the activities of a human data analyst. Functions use sensible defaults, or use no defaults in cases where the user must make a choice (e.g. a file path). {recipes} and {parnsip} enable data frames to be used every where in the modeling process. Data frames are often more convenient than working with matrices/vectors. It is consistent, so that what you learn about one function or package can be applied to another, and the number of special cases that you need to remember is as small as possible. Object orientated programming (mainly S3) for functions such as predict() provide a consistent interface to the user. broom::tidy() output is in a consistent format (data frame). List outputs provided by package-specific functions vary. It is composable, allowing you to solve complex problems by breaking them down into small pieces, supporting a rapid cycle of exploratory iteration to find the best solution. {recipes}, {parsnip}, {tune}, {dials}, etc are separate packages used in a tidy machine learning development workflow. It may seem inconvenient to have so many packages to perform specific tasks, but such a paradigm is helpful for decomposing the whole model design process, often making problems feel more manageable. It is inclusive, because the tidyverse is not just the collection of packages, but it is also the community of people who use them. Although the {tidyverse} and {tidymodels} are opinionated in their design, the developers are receptive to public feedback. "],["the-ames-housing-data.html", "Chapter 4 The Ames housing data", " Chapter 4 The Ames housing data Learning objectives: Explain why exploratory data analysis is an essential component of any modeling project. Recognize the Ames housing data - variables, context, and past cleaning. Explain when it makes sense to log-transform data. "],["pittsburgh-a-parallel-real-world-example.html", "4.1 Pittsburgh: a parallel real world example", " 4.1 Pittsburgh: a parallel real world example Conor Tompkins presented a fantastic overview of home sale price modeling by taking us through his recent project on Pittsburgh home sale price modeling, including discussions about his exploratory data analysis, motivations behind log-transforming sale data, and thoughts about inflation-adjusting historical sale prices. Code Repository here: https://github.com/conorotompkins/model_allegheny_house_sales Shiny app: https://conorotompkins.shinyapps.io/house_sale_estimator/ Cohort 1 Zoom chat logs 00:11:41 Tyler Grant Smith: does jon sound far away 00:11:44 Jonathan Trattner: yes 00:11:58 Yoni Sidi: Austin is far away 00:12:04 Tan Ho: very! 00:12:23 Conor Tompkins: https://conorotompkins.shinyapps.io/house_sale_estimator/ 00:12:26 Jon Harmon (jonthegeek): I ran out of USB slots temporarily so I&#39;m using my crappy microphone for a bit. 00:26:41 Yoni Sidi: what were the original motivations on cleaning the data, was it preset task driven or strictly data driven? 00:27:59 Tan Ho: oops sorry :P 00:28:35 Juan Guillermo: Hi everyone 00:28:42 Jon Harmon (jonthegeek): Welcome, Juan! 00:28:57 Juan Guillermo: thanks! 00:28:59 Darya Vanichkina: With school districts vs council districts, how does that work? I.e. can school districts span counties, and does it make sense to adjust house price based on whether or not the house is in a good school district? [not US-based, so not 100% sure how it works on the ground] 00:29:46 Jon Harmon (jonthegeek): I don&#39;t know about Pittsburgh, but school districts &amp; council districts are completely unrelated in Austin. 00:29:46 Jonathan Trattner: Did you save and document each of those iterations in the data cleaning set? Or were you just going through it looking for what you wanted? 00:30:06 Tan Ho: YAY JON 00:30:12 Jonathan Trattner: Congrats!! 00:30:38 Jon Harmon (jonthegeek): Thanks, it&#39;s exciting! 00:37:56 Tony ElHabr: Conor, did you think about including additional data sets, such as Zillow&#39;s forecasts? 00:38:04 Asmae Toumi: skimr is awesome, it can generate all sorts of summaries. you can also pipe it after a group_by 00:38:19 Darya Vanichkina: It also works on the command line! 00:38:32 Darya Vanichkina: Which is really impressive when working on cloud/HPC 00:39:12 David Severski: Zillow is pretty tight fisted about scraping their estimates. 00:39:28 pavitra: is esquisse comparable with skimr? 00:40:30 pavitra: cool.. thanks! 00:40:43 Kevin Kent: Ohh esquisse is sketch in French. That makes sense 00:41:05 Jon Harmon (jonthegeek): And now French speakers can laugh at me, Pavitra, AND Yoni&#39;s pronunciation! 00:41:44 pavitra: well, I pronounced it like a total desi - &quot;eskqueeeeez&quot;..you cannot top that, Jon! 00:42:11 Asmae Toumi: Really happy I tuned it tonight, didn’t know of priceR package to inflation adjust prices. Ive been doing it manually lol 00:42:24 Jon Harmon (jonthegeek): yeah, that&#39;s great even on its own for sure! 00:42:45 Jon Harmon (jonthegeek): He said GitHub so we need to make him give us the URL so we can put it in the book. 00:42:48 Tan Ho: I&#39;ve been making use of CANSIM to access stats Canada data on stuff, i&#39;m sure there&#39;s something comparable 00:43:03 pavitra: does this dataset include demographics also? 00:43:06 Tan Ho: https://github.com/stevecondylios/priceR 00:43:27 Joe Sydlowski: I feel like I put a lot of blind trust in packages like that. How much do you validate the functions when you find a new package? 00:43:31 Jon Harmon (jonthegeek): @pavitra I don&#39;t think this one did. 00:43:42 Asmae Toumi: Speaking of hockey bruins currently kicking Pittsburgh’s ass right now 00:43:55 Tan Ho: boston home prices kicking everyone&#39;s ass rn 00:44:00 Asmae Toumi: lmaooooo 00:44:07 Yoni Sidi: def check the code and the level of unit testing 00:44:39 Tyler Grant Smith: wouldnt neighborhod effect vary qith year of sale....gentrification etc 00:44:42 Jonathan Trattner: It is on CRAN for whatever that’s worth 00:44:49 Yoni Sidi: that&#39;s not worth much 00:44:54 Tan Ho: He&#39;s also in the R4DS slack channel 00:44:58 Yoni Sidi: on CRAN means they passed cmd check 00:45:05 Jonathan Trattner: Well yeah 00:45:09 Jonathan Trattner: But it also has some nice tests 00:45:12 Tony ElHabr: yoni is going to need to interview him before he approves of the package 00:45:15 David Severski: A little basic looking. https://github.com/stevecondylios/priceR/blob/master/R/adjust_for_inflation.R#L298-L321 00:45:22 Jonathan Trattner: Using api for world bank 00:46:08 David Severski: I tend to use indices direct from FRED for a lot of my own inflation conversion work. 00:46:31 Jon Harmon (jonthegeek): Yeah, it definitely depends how important exact numbers are to you. 00:46:50 Kevin Kent: I guess inflation would be a feature you’d have to forecast out if you wanted to get predictions for the future? But still noodling on that. 00:47:07 Tyler Grant Smith: it definitely is 00:47:29 Jon Harmon (jonthegeek): It looks like that package allowed for future inflation. 00:47:41 Jon Harmon (jonthegeek): (Conor commented out that part, but it showed in his code) 00:48:53 Kevin Kent: Oh nice. I feel like I run into that a lot in forecasting contexts - needing to be careful about the features and how uncertain they are in the future. 00:49:41 Scott Nestler: An aside since we have some sports fans in the group. Pine-Richland is where Phil Jurkovic, the former ND backup QB (who&#39;s now the starter at BC) is from. 00:49:54 Asmae Toumi: nice 00:52:29 Tyler Grant Smith: id definitely consider esp since a lot of pricing is done as $/sqft 00:52:38 David Severski: I wonder if lot sizes would discretize cleanly. Lot sizes tend to bin, right? 00:52:44 Tyler Grant Smith: did you engineer something like that 00:52:58 Jarad Jones: To go along with Jon’s question about log transforming…..how would you all decide to do that or not? 00:53:38 Scott Nestler: How did you collapse factor variables? With fct_lump_n() or something else? 00:53:46 David Severski: Jarad - Plotting out the distributions is something I try to do consistently, then look to transforms to get close-er to a normal distribution. 00:54:18 Tyler Grant Smith: not just log transform but box cox transforms more generally 00:54:22 Kevin Kent: I’d say it also depends on the assumptions of the model, and if they require normally distributed features. 00:54:22 Darya Vanichkina: Yes, like David - eyeball it :( 00:54:34 Tony ElHabr: yup. non negative is big use case 00:54:38 Asmae Toumi: In the words of the iconic Andrew German, “Log transform, kids. And don’t listen to people who tell you otherwise.” 00:54:45 Darya Vanichkina: Box Cox or Yeo Johnson 00:54:46 Asmae Toumi: link:https://statmodeling.stat.columbia.edu/2019/08/21/you-should-usually-log-transform-your-positive-data/ 00:54:56 Tyler Grant Smith: i would talk but i have lots of loud kids around me 00:55:05 David Severski: “And trust me about the sunscreen…” ;) 00:55:33 Asmae Toumi: Don’t forget the two finger rule for sunscreen also 00:55:37 Darya Vanichkina: Kevin, I think that because we could be comparing models which do/do not require normally distributed residuals I’d transform (and then compare) 00:55:42 Arjun Paudel: anytime you have a big tail 00:55:55 Tony ElHabr: also, if you&#39;re combining two predictions, I think log-transformed has good theoretical properties 00:56:13 Kevin Kent: Yeah that makes sense 00:56:47 Jarad Jones: That’s helpful, thanks! 00:56:52 Scott Nestler: That student was trying to maximize their leverage. 00:57:05 Jon Harmon (jonthegeek): They left and came back to the assignment but it&#39;s sometimes hard to see that. 00:57:13 Darya Vanichkina: It doesn’t - but you’re usually comparing the performance of the two 00:57:18 Darya Vanichkina: Right? 00:57:43 Tony ElHabr: yeah, I don&#39;t think it really needs it. but never hurts to try multiple methods 00:57:52 Tony ElHabr: tyler is the truth teller 00:58:00 Tony ElHabr: he got the kids to calm down for long enough 00:58:27 Tyler Grant Smith: yes that is one reason to do it 00:58:34 Tyler Grant Smith: theyre in the bath now 00:58:39 pavitra: for scientific assays, the dilutions are so large in range, I absolutely need to log-transform the data to make any sense of it 00:58:46 Jon Harmon (jonthegeek): ^^^ 00:59:00 Tony ElHabr: also, you look smarter if you log transform 00:59:04 Darya Vanichkina: LOL 00:59:07 Tony ElHabr: your audience will think you know what you&#39;re doing 00:59:10 shamsuddeen: lol 00:59:16 Kevin Kent: Lol fantastic 00:59:33 pavitra: john murdoch 00:59:33 Darya Vanichkina: I loved the RStudio conf talk where the FT journalist pros/cons of it 00:59:47 Darya Vanichkina: Yes, there are also “normal people”… 01:00:23 Tyler Grant Smith: lognormal people 01:01:35 Scott Nestler: 1 Full and 7 Half ??? 01:02:13 Jonathan Trattner: Maybe they’re complementary halves? 01:02:13 Tyler Grant Smith: shower in the bedroom but i cant be bothered to go to another room for #2 01:03:39 Tan Ho: doesn&#39;t your house have a three-urinal men&#39;s washroom separate from a three-stall women&#39;s washroom? 01:03:40 Darya Vanichkina: If possible, please, I’d love some documentation for all of the .R scripts on GitHub to make your thought process/prototyping clearer…. 01:03:43 Asmae Toumi: Is this on GitHub? I have a small aesthetic suggestion for the leaflet map so that the labels are on top of the color 01:04:15 Jon Harmon (jonthegeek): I believe it is and we&#39;ll make him share it in the channel/in the bookdown :) 01:04:17 Darya Vanichkina: I think it’s here? https://github.com/conorotompkins/model_allegheny_house_sales 01:04:28 Tan Ho: Yup, that&#39;s the one 01:04:43 Asmae Toumi: thanks 01:05:00 Darya Vanichkina: Sorry, need to run - thank you, everyone! 01:05:13 pavitra: has connor already developed any models on this data? 01:05:13 Jon Harmon (jonthegeek): See ya Darya! 01:05:26 Tan Ho: https://github.com/conorotompkins/model_allegheny_house_sales/tree/main/scripts/model @pavitra 01:05:28 Jon Harmon (jonthegeek): @pavitra: Yup! he predicts the price based on those settings. 01:05:47 pavitra: oh boy! neat 01:06:33 David Severski: Gotta run now. Thanks for the talk and the conversation! 01:07:30 Jon Harmon (jonthegeek): A numeric value between 0 and 1 or an integer greater or equal to one. If it&#39;s less than one then factor levels whose rate of occurrence in the training set are below threshold will be &quot;othered&quot;. If it&#39;s greater or equal to one then it&#39;s treated as a frequency and factor levels that occur less then threshold times will be &quot;othered&quot;. 01:09:13 Jon Harmon (jonthegeek): A logical. Should the step be skipped when the recipe is baked by bake.recipe()? While all operations are baked when prep.recipe() is run, some operations may not be able to be conducted on new data (e.g. processing the outcome variable(s)). Care should be taken when using skip = TRUE as it may affect the computations for subsequent operations 01:11:30 Jim Gruman: thankyou Conor 01:11:40 Kevin Kent: Nice job! Was helpful to talk through code and the concepts represented in it. 01:11:44 Tan Ho: thank you! that was awesome! 01:11:48 Jonathan Leslie: Thanks, Conor! 01:11:51 caroline: Thank you Conor 01:11:52 Joe Sydlowski: Thanks Conor! 01:11:56 Laurens Put: Thank you 01:11:59 Asmae Toumi: Conor that was awesome. I hope it ends up on tidytuesday 01:12:00 Jarad Jones: Nice job Conor, the whole end product is pretty impressive for a first shiny app! 01:12:00 pavitra: thanks a lot, Connor..i think you finished the purpose of the book "],["spending-our-data.html", "Chapter 5 Spending our data", " Chapter 5 Spending our data Learning objectives: Use {rsample} to split data into training and testing sets. Identify cases where stratified sampling is useful. Understand the difference between rsample::initial_time_split() and rsample::initial_split(). Understand the trade-offs between too little training data and too little testing data. Define a validation set of data. Explain why data should be split at the independent experimental unit level. "],["spending-our-data-1.html", "5.1 Spending our data", " 5.1 Spending our data The task of creating a useful model can be daunting. Thankfully, one can do so step-by-step. It can be helpful to sketch out your path, as Chanin Nantasenamat has done so: We’re going to zoom into the data splitting part. As the diagram shows, it is one of the earliest considerations in a model building workflow. The training set is the data that the model(s) learns from. It’s usually the majority of the data (~ 80-70% of the data), and you’ll be spending the bulk of your time working on fitting models to it. The test set is the data set aside for unbiased model validation once a candidate model(s) has been chosen. Unlike the training set, the test set is only looked at once. Why is it important to think about data splitting? You could do everything right, from cleaning the data, collecting features and picking a great model, but get bad results when you test the model on data it hasn’t seen before. If you’re in this predicament, the data splitting you’ve employed may be worth further investigation. "],["common-methods-for-splitting-data.html", "5.2 Common methods for splitting data", " 5.2 Common methods for splitting data Choosing how to conduct the split of the data into training and test sets may not be a trivial task. It depends on the data and the purpose. The most common type of sampling is known as random sampling and it is done readily in R using the rsample package with the initial_split()function. For the Ames housing dataset, the call would be: library(tidymodels) set.seed(123) data(ames) ames_split &lt;- initial_split(ames, prob = 0.80) ames_split ## &lt;Analysis/Assess/Total&gt; ## &lt;2198/732/2930&gt; The object ames_split is an rsplit object. To get the training and test results you can call on training() and test(): ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) "],["class-imbalance.html", "5.3 Class imbalance", " 5.3 Class imbalance In many instances, random splitting is not suitable. This includes datasets that contain class imbalance, meaning one class is dominated by another. Class imbalance is important to detect and take into consideration in data splitting. Performing random splitting on a dataset with severe class imbalance may cause the model to perform badly at validation. You want to avoid allocating the minority class disproportionately into the training or test set. The point is to have the same distribution across the training and test sets. Class imbalance can occur in differing degrees: Splitting methods suited for datasets containing class imbalance should be considered. Let’s consider a #Tidytuesday dataset on Himalayan expedition members, which Julia Silge recently explored here using {tidymodels}. library(tidyverse) library(skimr) members &lt;- read_csv(&quot;https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-22/members.csv&quot;) skim(members) Table 5.1: Data summary Name members Number of rows 76519 Number of columns 21 _______________________ Column type frequency: character 10 logical 6 numeric 5 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace expedition_id 0 1.00 9 9 0 10350 0 member_id 0 1.00 12 12 0 76518 0 peak_id 0 1.00 4 4 0 391 0 peak_name 15 1.00 4 25 0 390 0 season 0 1.00 6 7 0 5 0 sex 2 1.00 1 1 0 2 0 citizenship 10 1.00 2 23 0 212 0 expedition_role 21 1.00 4 25 0 524 0 death_cause 75413 0.01 3 27 0 12 0 injury_type 74807 0.02 3 27 0 11 0 Variable type: logical skim_variable n_missing complete_rate mean count hired 0 1 0.21 FAL: 60788, TRU: 15731 success 0 1 0.38 FAL: 47320, TRU: 29199 solo 0 1 0.00 FAL: 76398, TRU: 121 oxygen_used 0 1 0.24 FAL: 58286, TRU: 18233 died 0 1 0.01 FAL: 75413, TRU: 1106 injured 0 1 0.02 FAL: 74806, TRU: 1713 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist year 0 1.00 2000.36 14.78 1905 1991 2004 2012 2019 ▁▁▁▃▇ age 3497 0.95 37.33 10.40 7 29 36 44 85 ▁▇▅▁▁ highpoint_metres 21833 0.71 7470.68 1040.06 3800 6700 7400 8400 8850 ▁▁▆▃▇ death_height_metres 75451 0.01 6592.85 1308.19 400 5800 6600 7550 8830 ▁▁▂▇▆ injury_height_metres 75510 0.01 7049.91 1214.24 400 6200 7100 8000 8880 ▁▁▂▇▇ Let’s say we were interested in predicting the likelihood of survival or death for an expedition member. It would be a good idea to check for class imbalance: library(janitor) members %&gt;% tabyl(died) %&gt;% adorn_totals(&quot;row&quot;) ## died n percent ## FALSE 75413 0.98554607 ## TRUE 1106 0.01445393 ## Total 76519 1.00000000 We can see that nearly 99% of people survive their expedition. This dataset would be ripe for a sampling technique adept at handling such extreme class imbalance. This technique is called stratified sampling, in which “the training/test split is conducted separately within each class and then these subsamples are combined into the overall training and test set”. Operationally, this is done by using the strata argument inside initial_split(): set.seed(123) members_split &lt;- initial_split(members, prob = 0.80, strata = died) members_train &lt;- training(members_split) members_test &lt;- testing(members_split) "],["continuous-outcome-data.html", "5.4 Continuous outcome data", " 5.4 Continuous outcome data For continuous outcome data (e.g. costs), a stratified random sampling approach would involve conducting a 80/20 split within each quartile and then pool the results together. For the Ames housing dataset, the call would look like this: set.seed(123) ames_split &lt;- initial_split(ames, prob = 0.80, strata = Sale_Price) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) "],["time-series-data.html", "5.5 Time series data", " 5.5 Time series data For time series data where you’d want to allocate data to the training set/test set depending on a sorted order, you can use initial_time_split() which works similarly to initial_split(). The prop argument can be used to specify what proportion of the first part of data should be used as the training set. data(drinks) drinks_split &lt;- initial_time_split(drinks) train_data &lt;- training(drinks_split) test_data &lt;- testing(drinks_split) The lag argument can specify a lag period to use between the training and test set. This is useful if lagged predictors will be used during training and testing. drinks_lag_split &lt;- initial_time_split(drinks, lag = 12) train_data_lag &lt;- training(drinks_lag_split) test_data_lag &lt;- testing(drinks_lag_split) c(max(train_data_lag$date), min(test_data_lag$date)) ## [1] &quot;2011-03-01&quot; &quot;2010-04-01&quot; "],["multi-level-data.html", "5.6 Multi-level data", " 5.6 Multi-level data It’s important to figure out what the independent experimental unit is in your data. In the Ames dataset, there is one row per house and so houses and their properties are considered to be independent of one another. In other datasets, there may be multiple rows per experimental unit (e.g. as in patients who are measured multiple times across time). This has implications for data splitting. To avoid data from the same experimental unit being in both the training and test set, split along the independent experimental units such that X% of experimental units are in the training set. "],["what-proportion-should-be-used.html", "5.7 What proportion should be used?", " 5.7 What proportion should be used? Where does the 80/20 training/testing split come from?&mdash; Asmae Toumi (@asmae_toumi) January 31, 2021 Some people said the 80/20 split comes from the Pareto principle/distribution or the power law. Some said because it works nicely with 5-fold cross-validation (which we will see in the later chapters). I believe the point is to use enough data in the training set to allow for solid parameter estimation but not too much that it hurts performance. 80/20 or 70/30 seems reasonable for most problems at hand, as it’s what is widely used. Max Kuhn notes that a test set is almost always a good idea, and it should only be avoided when the data is “pathologically small”. "],["summary.html", "5.8 Summary", " 5.8 Summary Data splitting is an important part of a modeling workflow as it impacts model validity and performance. The most common splitting technique is random splitting. Some data, such as time-series or multi-level data require a different data splitting technique called stratified sampling. The rsample package contains many functions that can perform random splitting and stratified splitting. We will learn more about how to remedy certain issues such as class imbalance, bias and overfitting in Chapter 10. 5.8.1 References Tidy modeling with R by Max Kuhn and Julia Silge: https://www.tmwr.org/splitting.html Feature Engineering and Selection: A Practical Approach for Predictive Models by Max Kuhn and Kjell Johnson: https://bookdown.org/max/FES/ Handle class imbalance in #TidyTuesday climbing expedition data with tidymodels: https://juliasilge.com/blog/himalayan-climbing/ Data preparation and feature engineering for machine learning: https://developers.google.com/machine-learning/data-prep/construct/sampling-splitting/imbalanced-data How to Build a Machine Learning Model by Chanin Nantasenamat: https://towardsdatascience.com/how-to-build-a-machine-learning-model-439ab8fb3fb1 "],["feature-engineering-with-recipes.html", "Chapter 6 Feature engineering with recipes", " Chapter 6 Feature engineering with recipes Learning objectives: Jon will prefill these. "],["slide-1-title.html", "6.1 Slide 1 Title", " 6.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title.html", "6.2 Slide 2 Title", " 6.2 Slide 2 Title Put the content of your second slide here. "],["fitting-models-with-parsnip.html", "Chapter 7 Fitting models with parsnip", " Chapter 7 Fitting models with parsnip Learning objectives: Jon will prefill these. "],["slide-1-title-1.html", "7.1 Slide 1 Title", " 7.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-1.html", "7.2 Slide 2 Title", " 7.2 Slide 2 Title Put the content of your second slide here. "],["a-model-workflow.html", "Chapter 8 A model workflow", " Chapter 8 A model workflow Learning objectives: Jon will prefill these. "],["slide-1-title-2.html", "8.1 Slide 1 Title", " 8.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-2.html", "8.2 Slide 2 Title", " 8.2 Slide 2 Title Put the content of your second slide here. "],["judging-model-effectiveness.html", "Chapter 9 Judging model effectiveness", " Chapter 9 Judging model effectiveness Learning objectives: Jon will prefill these. "],["slide-1-title-3.html", "9.1 Slide 1 Title", " 9.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-3.html", "9.2 Slide 2 Title", " 9.2 Slide 2 Title Put the content of your second slide here. "],["review-of-chapters-4-9.html", "Review of chapters 4-9", " Review of chapters 4-9 Learning objectives: Jon will prefill these. "],["slide-1-title-4.html", "9.3 Slide 1 Title", " 9.3 Slide 1 Title Put the content of your slide here. "],["slide-2-title-4.html", "9.4 Slide 2 Title", " 9.4 Slide 2 Title Put the content of your second slide here. "],["resampling-for-evaluating-performance.html", "Chapter 10 Resampling for evaluating performance", " Chapter 10 Resampling for evaluating performance Learning objectives: Jon will prefill these. "],["slide-1-title-5.html", "10.1 Slide 1 Title", " 10.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-5.html", "10.2 Slide 2 Title", " 10.2 Slide 2 Title Put the content of your second slide here. "],["comparing-models-with-resampling.html", "Chapter 11 Comparing models with resampling", " Chapter 11 Comparing models with resampling Learning objectives: Jon will prefill these. "],["slide-1-title-6.html", "11.1 Slide 1 Title", " 11.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-6.html", "11.2 Slide 2 Title", " 11.2 Slide 2 Title Put the content of your second slide here. "],["model-tuning-and-the-dangers-of-overfitting.html", "Chapter 12 Model tuning and the dangers of overfitting", " Chapter 12 Model tuning and the dangers of overfitting Learning objectives: Jon will prefill these. "],["slide-1-title-7.html", "12.1 Slide 1 Title", " 12.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-7.html", "12.2 Slide 2 Title", " 12.2 Slide 2 Title Put the content of your second slide here. "],["grid-search.html", "Chapter 13 Grid search", " Chapter 13 Grid search Learning objectives: Jon will prefill these. "],["slide-1-title-8.html", "13.1 Slide 1 Title", " 13.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-8.html", "13.2 Slide 2 Title", " 13.2 Slide 2 Title Put the content of your second slide here. "],["iterative-search.html", "Chapter 14 Iterative search", " Chapter 14 Iterative search Learning objectives: Jon will prefill these. "],["slide-1-title-9.html", "14.1 Slide 1 Title", " 14.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-9.html", "14.2 Slide 2 Title", " 14.2 Slide 2 Title Put the content of your second slide here. "],["review-of-chapters-10-14.html", "Review of chapters 10-14", " Review of chapters 10-14 Learning objectives: Jon will prefill these. "],["slide-1-title-10.html", "14.3 Slide 1 Title", " 14.3 Slide 1 Title Put the content of your slide here. "],["slide-2-title-10.html", "14.4 Slide 2 Title", " 14.4 Slide 2 Title Put the content of your second slide here. "]]
