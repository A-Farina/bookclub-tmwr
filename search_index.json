[["index.html", "Tidy Modeling with R Book Club Welcome", " Tidy Modeling with R Book Club The R4DS Online Learning Community 2021-01-26 Welcome This is a companion for the book Tidy Modeling with R by Max Kuhn and Julia Silge. This companion is available at r4ds.io/tmwr. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book. This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["software-for-modeling.html", "Chapter 1 Software for modeling", " Chapter 1 Software for modeling Learning objectives: Recognize the principles around which the {tidymodels} packages were designed. Classify models as descriptive, inferential, and/or predictive. Define descriptive model. Define inferential model. Define predictive model. Differentiate between supervised and unsupervised models. Differentiate between regression and classification models. Differentiate between quantitative and qualitative data. Understand the roles that data can have in an analysis. Apply the data science process. Recognize the phases of modeling. "],["the-pit-of-success.html", "1.1 The pit of success", " 1.1 The pit of success {tidymodels} aims to help us fall into the Pit of Success: The Pit of Success: in stark contrast to a summit, a peak, or a journey across a desert to find victory through many trials and surprises, we want our customers to simply fall into winning practices by using our platform and frameworks. Avoid confusion: Software should facilitate proper usage. Avoid mistakes: Software should make it easy for users to do the right thing. "],["types-of-models.html", "1.2 Types of models", " 1.2 Types of models Descriptive models: Describe or illustrate characteristics of data. Inferential models: Make some statement of truth regarding a predefined conjecture or idea. Usually delayed feedback between inference and actual result. Predictive models: Produce the most accurate possible prediction for new data. Estimation (“How much?”) rather than inference (“Will it?”). Mechanistic models are derived using first principles to produce a model equation that is dependent on assumptions. Empirically driven models have more vague assumptions, and are derived directly from the data. "],["terminology.html", "1.3 Terminology", " 1.3 Terminology Unsupervised models learn patterns, clusters, etc, but lack an outcome variable. Examples: PCA, clustering, autoencoders. Supervised models have an outcome variable. Examples: linear regression, neural networks. Regression: numeric outcome Classification: ordered or unordered qualitative values. Quantitative data: numbers. Qualitative (nominal) data: non-numbers. Data can have different roles in analyses: Outcomes (labels, endpoints, dependent variables): the value being predicted in supervised models. Predictors (independent variables): the variables used to predict the outcome. "],["the-data-analysis-process.html", "1.4 The data analysis process", " 1.4 The data analysis process The data science process (from R for Data Science by Wickham and Grolemund. "],["the-modeling-process.html", "1.5 The modeling process", " 1.5 The modeling process The modeling process. Exploratory data analysis: Explore the data to see what they might tell you. Don’t underestimate the time you’ll need for cleaning the data. It’s the step that the ’verse is named after! Take time to understand the data. Develop clear expectations of the goal of your model and how performance will be judged (Chapter 9). Feature engineering: Create specific model terms. Covered in Chapter 6. Model tuning and selection: Generate a variety of models and compare performance. Model evaluation: Use EDA-like analyses to choose the best model for your situation. "],["a-tidyverse-primer.html", "Chapter 2 A tidyverse primer", " Chapter 2 A tidyverse primer Learning objectives: List the tidyverse design principles. Explain what it means for the tidyverse to be designed for humans. Describe how reusing existing data structures can make functions easier to work with. Explain what it means for a set of functions to be designed for the pipe. Explain what it means for function to be designed for functional programming. List some differences between a tibble and a base data.frame. Recognize how to use the tidyverse to read and wrangle data. "],["tidyverse-design-principles.html", "2.1 Tidyverse design Principles", " 2.1 Tidyverse design Principles The tidyverse has four core design principles: Human centered: Designed to promote human usability. Consistent: Learning how to use one function or package is as similar as another. Composable: Easily breakdown data challenges into smaller components with exploratory tools to find the best solution. Inclusive: Fostering a community of like-minded users (e.g. #rstats) "],["design-for-humans.html", "2.2 Design for Humans", " 2.2 Design for Humans “Programs must be written for people to read, and only incidentally for machines to execute.” - Hal Abelson The tidyverse offers packages that are easily readable and understood by humans. It enables them to more easily achieve their programming goals. Consider the mtcars dataset, which comprises fuel consumption and 10 aspects of autombile design and performance from 1973-1974. Previewing the first six rows of the data, we see: ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 If we wanted to arrange these in ascending order based on the mpg and gear variables, how could we do this? The function arrange(), in the dplyr package of the tidyverse, takes a data frame and column names as such: arrange(.data = mtcars, gear, mpg) arrange(), and other tidyverse functions, use names that are descriptive and explicit. For general methods, there is a focus on verbs, as seen with the functions pivot_longer() and pivot_wider() in the tidyr package. "],["reusing-existing-data-structures.html", "2.3 Reusing existing data structures", " 2.3 Reusing existing data structures “You don’t have to reinvent the wheel, just attach it to a new wagon.” - Mark McCormack There are many different data types in R, such as matrices, lists, and data frames.1 A typical function would take in data of some form, conduct an operation, and return the result. tidyverse functions most often operate on data structures called tibbles. Traditional data frames can represent different data types in each column, and multiple values in each row. Tibbles are a special data frame that have additional properties helpful for data analysis. Example: list-columns boot_samp &lt;- rsample::bootstraps(mtcars, times = 3) boot_samp ## # Bootstrap sampling ## [90m# A tibble: 3 x 2[39m ## splits id ## [3m[90m&lt;list&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m ## [90m1[39m [90m&lt;split [32/13]&gt;[39m Bootstrap1 ## [90m2[39m [90m&lt;split [32/11]&gt;[39m Bootstrap2 ## [90m3[39m [90m&lt;split [32/14]&gt;[39m Bootstrap3 class(boot_samp) ## [1] &quot;bootstraps&quot; &quot;rset&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; The above example shows how to create bootstrap resamples of the data frame mtcars. It returns a tibble with a splits column that defines the resampled data sets. This function inherits data frame and tibble methods so other functions that operate on those data structures can be used. For a more detailed discussion, see Hadley Wickham’s Advanced R↩︎ "],["designed-for-the-pipe.html", "2.4 Designed for the pipe", " 2.4 Designed for the pipe The pipe operator, %&gt;%, comes from the magrittr package by Stefan Milton Bache, and is used to chain together a sequence of R functions. More specifically, the pipe operator uses the value of the object on the left-hand side of the operator as the first argument on the operator’s right-hand side. The pipe allows for highly readable code. Consider wanting to sort the mtcars dataset by the number of gears (gear) and then select the first ten rows. How would you do that? cars_arranged &lt;- arrange(mtcars, gear) cars_selected &lt;- slice(cars_arranged, 1:10) # more compactly cars_selected &lt;- slice(arrange(mtcars, gear), 1:10) Using the pipe to substitute the left-hand side of the operator with the first argument on the right-hand side, we can get the same result as follows: cars_selected &lt;- mtcars %&gt;% arrange(gear) %&gt;% slice(1:10) This approach with the pipe works because all the functions return the same data structure (a tibble/data frame) which is the first argument of the next function. Whenever possible, create functions that can be incorporated into a pipeline of operations. "],["designed-for-functional-programming.html", "2.5 Designed for Functional Programming", " 2.5 Designed for Functional Programming Functional Programming is an approach to replace iterative (i.e. for) loops. Consider the case where you may want two times the square root of the mpg for each car in mtcars. You could do this with a for loop as follows: n &lt;- nrow(mtcars) roots &lt;- rep(NA_real_, n) for (car in 1:n) { roots[car] &lt;- 2 * sqrt(mtcars$mpg[car]) } You could also write a function to do the computations. In functional programming, it’s important that the function does not have any side effects and the output only depends on the inputs. For example, the function my_sqrt() takes in a car’s mpg and a weight by which to multiply the square root. my_sqrt &lt;- function(mpg, weight) { weight * sqrt(mpg) } Using the purrr package, we can forgo the for loop and use the map() family of functions which use the basic syntax of map(vector, function). Below, we are applying the my_sqrt() function, with a weight of 2, to the first three elements of mtcars$mpg. User supplied functions can be declared by prefacing it with ~ (pronounced “twiddle”). By default, map() returns a list. If you know the class of a function’s output, you can use special suffixes. A character output, for example, would used by map_chr(), a double by map_dbl(), and a logical by map_lgl(). map( .x = head(mtcars$mpg, 3), ~ my_sqrt( mpg = .x, weight = 2 ) ) ## [[1]] ## [1] 9.165151 ## ## [[2]] ## [1] 9.165151 ## ## [[3]] ## [1] 9.549869 map() functions can be used with 2 inputs, by specifying map2() Requires arguments .x and .y map2( .x = head(mtcars$mpg, 3), .y = c(1,2,3), ~ my_sqrt( mpg = .x, weight = .y ) ) ## [[1]] ## [1] 4.582576 ## ## [[2]] ## [1] 9.165151 ## ## [[3]] ## [1] 14.3248 "],["tibbles-vs-data-frames.html", "2.6 Tibbles vs. Data Frames", " 2.6 Tibbles vs. Data Frames A tibble is a special type of data frame with some additional properties. Specifically: Tibbles work with column names that are not syntactically valid variable names. data.frame(`this does not work` = 1:2, oops = 3:4) ## this.does.not.work oops ## 1 1 3 ## 2 2 4 tibble(`this does work, though` = 1:2, `woohoo!` = 3:4) ## [90m# A tibble: 2 x 2[39m ## `this does work, though` `woohoo!` ## [3m[90m&lt;int&gt;[39m[23m [3m[90m&lt;int&gt;[39m[23m ## [90m1[39m 1 3 ## [90m2[39m 2 4 Tibbles prevent partial matching of arguments to avoid accidental errors df &lt;- data.frame(partial = 1:5) tbbl &lt;- tibble(partial = 1:5) df$part ## [1] 1 2 3 4 5 tbbl$part ## Warning: Unknown or uninitialised column: `part`. ## NULL Tibbles prevent dimension dropping, so subsetting data into a single column will never return a vector. df[, &quot;partial&quot;] ## [1] 1 2 3 4 5 tbbl[, &quot;partial&quot;] ## [90m# A tibble: 5 x 1[39m ## partial ## [3m[90m&lt;int&gt;[39m[23m ## [90m1[39m 1 ## [90m2[39m 2 ## [90m3[39m 3 ## [90m4[39m 4 ## [90m5[39m 5 Tibbles allow for list-columns, which can be a powerful tool when working with the purrr package. template_list &lt;- list(a = 1, b = 2, c = 3, d = 4, e = 5) data.frame(col = 1:5, list_col = template_list) ## col list_col.a list_col.b list_col.c list_col.d list_col.e ## 1 1 1 2 3 4 5 ## 2 2 1 2 3 4 5 ## 3 3 1 2 3 4 5 ## 4 4 1 2 3 4 5 ## 5 5 1 2 3 4 5 tibble(col = 1:5, list_col = template_list) ## [90m# A tibble: 5 x 2[39m ## col list_col ## [3m[90m&lt;int&gt;[39m[23m [3m[90m&lt;named list&gt;[39m[23m ## [90m1[39m 1 [90m&lt;dbl [1]&gt;[39m ## [90m2[39m 2 [90m&lt;dbl [1]&gt;[39m ## [90m3[39m 3 [90m&lt;dbl [1]&gt;[39m ## [90m4[39m 4 [90m&lt;dbl [1]&gt;[39m ## [90m5[39m 5 [90m&lt;dbl [1]&gt;[39m "],["how-to-read-and-wrangle-data.html", "2.7 How to read and wrangle data", " 2.7 How to read and wrangle data The following example shows how to use the tidyverse to read in data (with the readr package) and easily manipulate it (using the dplyr and lubridate packages). We will walk through these steps during our meeting. library(tidyverse) library(lubridate) url &lt;- &quot;http://bit.ly/raw-train-data-csv&quot; all_stations &lt;- # Step 1: Read in the data. readr::read_csv(url) %&gt;% # Step 2: filter columns and rename stationname dplyr::select(station = stationname, date, rides) %&gt;% # Step 3: Convert the character date field to a date encoding. # Also, put the data in units of 1K rides dplyr::mutate(date = lubridate::mdy(date), rides = rides / 1000) %&gt;% # Step 4: Summarize the multiple records using the maximum. dplyr::group_by(date, station) %&gt;% dplyr::summarize(rides = max(rides), .groups = &quot;drop&quot;) head(all_stations, 10) ## [90m# A tibble: 10 x 3[39m ## date station rides ## [3m[90m&lt;date&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m 2001-01-01 18th 0 ## [90m 2[39m 2001-01-01 35-Bronzeville-IIT 0.448 ## [90m 3[39m 2001-01-01 35th/Archer 0.318 ## [90m 4[39m 2001-01-01 43rd 0.211 ## [90m 5[39m 2001-01-01 47th-Dan Ryan 0.787 ## [90m 6[39m 2001-01-01 47th-South Elevated 0.427 ## [90m 7[39m 2001-01-01 51st 0.364 ## [90m 8[39m 2001-01-01 54th/Cermak 0 ## [90m 9[39m 2001-01-01 63rd-Dan Ryan 1.37 ## [90m10[39m 2001-01-01 69th 2.37 “This pipeline of operations illustrates why the tidyverse is popular. A series of data manipulations is used that have simple and easy to understand user interfaces; the series is bundled together in a streamlined and readable way. The focus is on how the user interacts with the software. This approach enables more people to learn R and achieve their analysis goals, and adopting these same principles for modeling in R has the same benefits.” - Max Kuhn and Julia Silge in Tidy Modeling with R "],["a-review-of-r-modeling-fundamentals.html", "Chapter 3 A review of R modeling fundamentals", " Chapter 3 A review of R modeling fundamentals Learning objectives: Specify model terms using the R formula syntax. List conveniences for modeling that are supported by the R formula syntax. Use anova() to compare models. Use summary() to inspect a model. Use predict() to generate new predictions from a model. List the three purposes that the R model formula serves. Recognize how the design for humans rubric is applied to {tidymodels} packages. Use broom::tidy() to standardize the structure of R objects. Use the {tidyverse} along with base modeling functions like lm() to produce multiple models at once. "],["r-formula-syntax.html", "3.1 R formula syntax", " 3.1 R formula syntax We’ll use the trees data set provided in {modeldata} (loaded with {tidymodels}) for demonstration purposes. Tree girth (in inches), height (in feet), and volume (in cubic feet) are provided. (Girth is somewhat like a measure of diameter.) library(tidyverse) library(tidymodels) theme_set(theme_minimal(base_size = 14)) data(trees) trees &lt;- as_tibble(trees) trees ## [90m# A tibble: 31 x 3[39m ## Girth Height Volume ## [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m 8.3 70 10.3 ## [90m 2[39m 8.6 65 10.3 ## [90m 3[39m 8.8 63 10.2 ## [90m 4[39m 10.5 72 16.4 ## [90m 5[39m 10.7 81 18.8 ## [90m 6[39m 10.8 83 19.7 ## [90m 7[39m 11 66 15.6 ## [90m 8[39m 11 75 18.2 ## [90m 9[39m 11.1 80 22.6 ## [90m10[39m 11.2 75 19.9 ## [90m# … with 21 more rows[39m Note that there is an analytical way to calculate tree volume from measures of diameter and height. We observe that Girth is strongly correlated with Volume trees %&gt;% corrr::correlate() ## [90m# A tibble: 3 x 4[39m ## term Girth Height Volume ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m Girth [31mNA[39m 0.519 0.967 ## [90m2[39m Height 0.519 [31mNA[39m 0.598 ## [90m3[39m Volume 0.967 0.598 [31mNA[39m Shame on you 😉 if you didn’t guess I would make a scatter plot given a data set with two variables. trees %&gt;% ggplot(aes(x = Girth, y = Height)) + geom_point(aes(size = Volume)) We can fit a linear regression model to predict Volume as a function of the other two features, using the formula syntax to save us from some typing. reg_fit &lt;- lm(Volume ~ ., data = trees) reg_fit ## ## Call: ## lm(formula = Volume ~ ., data = trees) ## ## Coefficients: ## (Intercept) Girth Height ## -57.9877 4.7082 0.3393 How would you write this without the formula syntax? If we want to get fancy with our pipes (%&gt;%), then we should wrap our formula with formula() trees %&gt;% lm(formula(Volume ~ .), data = .) ## ## Call: ## lm(formula = formula(Volume ~ .), data = .) ## ## Coefficients: ## (Intercept) Girth Height ## -57.9877 4.7082 0.3393 Interaction terms are easy to generate. inter_fit &lt;- lm(Volume ~ Girth * Height, data = trees) inter_fit ## ## Call: ## lm(formula = Volume ~ Girth * Height, data = trees) ## ## Coefficients: ## (Intercept) Girth Height Girth:Height ## 69.3963 -5.8558 -1.2971 0.1347 Same goes for polynomial terms. poly_fit &lt;- lm(Volume ~ Girth + I(Girth^2) + Height, data = trees) poly_fit ## ## Call: ## lm(formula = Volume ~ Girth + I(Girth^2) + Height, data = trees) ## ## Coefficients: ## (Intercept) Girth I(Girth^2) Height ## -9.9204 -2.8851 0.2686 0.3764 Excluding columns is intuitive. no_height_fit &lt;- lm(Volume ~ . - Height, data = trees) no_height_fit ## ## Call: ## lm(formula = Volume ~ . - Height, data = trees) ## ## Coefficients: ## (Intercept) Girth ## -36.943 5.066 The intercept term can be removed conveniently. no_intercept_fit &lt;- lm(Volume ~ . + 0, data = trees) no_intercept_fit ## ## Call: ## lm(formula = Volume ~ . + 0, data = trees) ## ## Coefficients: ## Girth Height ## 5.0440 -0.4773 To illustrate another convenience provided by formulas, let’s add a categorical column. trees2 &lt;- trees set.seed(42) trees2$group = sample(toupper(letters[1:4]), size = nrow(trees2), replace = TRUE) trees2 ## [90m# A tibble: 31 x 4[39m ## Girth Height Volume group ## [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m ## [90m 1[39m 8.3 70 10.3 A ## [90m 2[39m 8.6 65 10.3 A ## [90m 3[39m 8.8 63 10.2 A ## [90m 4[39m 10.5 72 16.4 A ## [90m 5[39m 10.7 81 18.8 B ## [90m 6[39m 10.8 83 19.7 D ## [90m 7[39m 11 66 15.6 B ## [90m 8[39m 11 75 18.2 B ## [90m 9[39m 11.1 80 22.6 A ## [90m10[39m 11.2 75 19.9 D ## [90m# … with 21 more rows[39m Encoding the categories as separate features is done auto-magically with the formula syntax. dummy_fit &lt;- lm(Volume ~ ., data = trees2) dummy_fit ## ## Call: ## lm(formula = Volume ~ ., data = trees2) ## ## Coefficients: ## (Intercept) Girth Height groupB groupC groupD ## -55.2921 4.6932 0.3093 -1.8367 -0.0497 0.6462 Under the hood, this is done by model.matrix(). model.matrix(Volume ~ ., data = trees2) %&gt;% head(10) ## (Intercept) Girth Height groupB groupC groupD ## 1 1 8.3 70 0 0 0 ## 2 1 8.6 65 0 0 0 ## 3 1 8.8 63 0 0 0 ## 4 1 10.5 72 0 0 0 ## 5 1 10.7 81 1 0 0 ## 6 1 10.8 83 0 0 1 ## 7 1 11.0 66 1 0 0 ## 8 1 11.0 75 1 0 0 ## 9 1 11.1 80 0 0 0 ## 10 1 11.2 75 0 0 1 3.1.1 Recap Purposes of R model formula: The formula defines the columns that are used by the model. The standard R machinery uses the formula to encode the columns into an appropriate format. The roles of the columns are defined by the formula. "],["inspecting-and-developing-models.html", "3.2 Inspecting and developing models", " 3.2 Inspecting and developing models Being the sound analysts that we are, we should check if the assumptions of linear regression are violated. The plot() generic function has a specific method for lm objects that generates various diagnostic plots. par(mfrow = c(1, 2)) plot(reg_fit, which = c(1, 2)) The second plot does not show any strong violation of the normality assumption. However, the first plot shows a violation of the linearity assumption (that there is a linear relationship between the response variable and the predictors). If the assumption were satisfied, the smooth red line would be like a straight horizontal line at y=0. Note that there is a {ggplot2} way to generate the same plots. library(ggfortify) autoplot(reg_fit, which = c(1, 2)) But what about the coefficients? summary(reg_fit) ## ## Call: ## lm(formula = Volume ~ ., data = trees) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.4065 -2.6493 -0.2876 2.2003 8.4847 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -57.9877 8.6382 -6.713 2.75e-07 *** ## Girth 4.7082 0.2643 17.816 &lt; 2e-16 *** ## Height 0.3393 0.1302 2.607 0.0145 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.882 on 28 degrees of freedom ## Multiple R-squared: 0.948, Adjusted R-squared: 0.9442 ## F-statistic: 255 on 2 and 28 DF, p-value: &lt; 2.2e-16 Use {broom} for a tidy version. library(broom) reg_fit %&gt;% tidy() ## [90m# A tibble: 3 x 5[39m ## term estimate std.error statistic p.value ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m (Intercept) -[31m58[39m[31m.[39m[31m0[39m 8.64 -[31m6[39m[31m.[39m[31m71[39m 2.75[90me[39m[31m- 7[39m ## [90m2[39m Girth 4.71 0.264 17.8 8.22[90me[39m[31m-17[39m ## [90m3[39m Height 0.339 0.130 2.61 1.45[90me[39m[31m- 2[39m reg_fit %&gt;% glance() %&gt;% glimpse() ## Rows: 1 ## Columns: 12 ## $ r.squared [3m[90m&lt;dbl&gt;[39m[23m 0.94795 ## $ adj.r.squared [3m[90m&lt;dbl&gt;[39m[23m 0.9442322 ## $ sigma [3m[90m&lt;dbl&gt;[39m[23m 3.881832 ## $ statistic [3m[90m&lt;dbl&gt;[39m[23m 254.9723 ## $ p.value [3m[90m&lt;dbl&gt;[39m[23m 1.071238e-18 ## $ df [3m[90m&lt;dbl&gt;[39m[23m 2 ## $ logLik [3m[90m&lt;dbl&gt;[39m[23m -84.45499 ## $ AIC [3m[90m&lt;dbl&gt;[39m[23m 176.91 ## $ BIC [3m[90m&lt;dbl&gt;[39m[23m 182.6459 ## $ deviance [3m[90m&lt;dbl&gt;[39m[23m 421.9214 ## $ df.residual [3m[90m&lt;int&gt;[39m[23m 28 ## $ nobs [3m[90m&lt;int&gt;[39m[23m 31 {purrr} and {dplyr} can help you scale up your modeling process. We can compare all of the models we made before. list( &#39;reg&#39; = reg_fit, &#39;inter&#39; = inter_fit, &#39;poly&#39; = poly_fit, &#39;no_height&#39; = no_height_fit, &#39;no_intercept&#39; = no_intercept_fit ) %&gt;% map_dfr(glance, .id = &#39;id&#39;) %&gt;% select(id, adj.r.squared) %&gt;% arrange(desc(adj.r.squared)) ## [90m# A tibble: 5 x 2[39m ## id adj.r.squared ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m poly 0.975 ## [90m2[39m inter 0.973 ## [90m3[39m no_intercept 0.968 ## [90m4[39m reg 0.944 ## [90m5[39m no_height 0.933 We observe that the polynomial fit is the best. We can create models for each group in trees2. reg_fits &lt;- trees2 %&gt;% group_nest(group) %&gt;% mutate( fit = map(data, ~ lm(formula(Volume ~ .), data = .x)), tidied = map(fit, tidy), glanced = map(fit, glance), augmented = map(fit, augment) ) .select_unnest &lt;- function(data, ...) { data %&gt;% select(group, ...) %&gt;% unnest(...) } reg_fits %&gt;% .select_unnest(tidied) ## [90m# A tibble: 12 x 6[39m ## group term estimate std.error statistic p.value ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m A (Intercept) -[31m44[39m[31m.[39m[31m6[39m 17.5 -[31m2[39m[31m.[39m[31m55[39m 0.031[4m2[24m ## [90m 2[39m A Girth 4.21 0.477 8.83 0.000[4m0[24m[4m0[24m[4m9[24m98 ## [90m 3[39m A Height 0.250 0.294 0.849 0.418 ## [90m 4[39m B (Intercept) -[31m66[39m[31m.[39m[31m1[39m 13.9 -[31m4[39m[31m.[39m[31m74[39m 0.017[4m8[24m ## [90m 5[39m B Girth 4.16 0.704 5.91 0.009[4m6[24m[4m9[24m ## [90m 6[39m B Height 0.520 0.123 4.24 0.024[4m0[24m ## [90m 7[39m C (Intercept) -[31m86[39m[31m.[39m[31m4[39m 90.5 -[31m0[39m[31m.[39m[31m954[39m 0.410 ## [90m 8[39m C Girth 4.83 0.747 6.47 0.007[4m4[24m[4m8[24m ## [90m 9[39m C Height 0.680 1.20 0.567 0.611 ## [90m10[39m D (Intercept) -[31m46[39m[31m.[39m[31m3[39m 14.8 -[31m3[39m[31m.[39m[31m14[39m 0.034[4m9[24m ## [90m11[39m D Girth 6.03 0.372 16.2 0.000[4m0[24m[4m8[24m[4m5[24m2 ## [90m12[39m D Height -[31m0[39m[31m.[39m[31m0[39m[31m26[4m8[24m[39m 0.214 -[31m0[39m[31m.[39m[31m125[39m 0.906 reg_fits %&gt;% .select_unnest(glanced) ## [90m# A tibble: 4 x 13[39m ## group r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m1[39m A 0.960 0.951 3.81 107. 5.27[90me[39m[31m-7[39m 2 -[31m31[39m[31m.[39m[31m3[39m 70.7 72.6 ## [90m2[39m B 0.935 0.891 2.20 21.5 1.66[90me[39m[31m-2[39m 2 -[31m11[39m[31m.[39m[31m2[39m 30.3 29.5 ## [90m3[39m C 0.946 0.910 4.06 26.2 1.26[90me[39m[31m-2[39m 2 -[31m14[39m[31m.[39m[31m8[39m 37.7 36.8 ## [90m4[39m D 0.990 0.985 2.80 194. 1.04[90me[39m[31m-4[39m 2 -[31m15[39m[31m.[39m[31m2[39m 38.4 38.2 ## [90m# … with 3 more variables: deviance [3m[90m&lt;dbl&gt;[90m[23m, df.residual [3m[90m&lt;int&gt;[90m[23m, nobs [3m[90m&lt;int&gt;[90m[23m[39m reg_fits %&gt;% .select_unnest(augmented) ## [90m# A tibble: 31 x 10[39m ## group Volume Girth Height .fitted .resid .std.resid .hat .sigma .cooksd ## [3m[90m&lt;chr&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m [3m[90m&lt;dbl&gt;[39m[23m ## [90m 1[39m A 10.3 8.3 70 7.81 2.49 0.758 0.252 3.91 0.064[4m5[24m ## [90m 2[39m A 10.3 8.6 65 7.82 2.48 0.769 0.283 3.90 0.078[4m0[24m ## [90m 3[39m A 10.2 8.8 63 8.17 2.03 0.687 0.395 3.93 0.103 ## [90m 4[39m A 16.4 10.5 72 17.6 -[31m1[39m[31m.[39m[31m18[39m -[31m0[39m[31m.[39m[31m332[39m 0.134 4.01 0.005[4m6[24m[4m6[24m ## [90m 5[39m A 22.6 11.1 80 22.1 0.500 0.192 0.534 4.03 0.014[4m2[24m ## [90m 6[39m A 19.1 12 75 24.6 -[31m5[39m[31m.[39m[31m54[39m -[31m1[39m[31m.[39m[31m56[39m 0.123 3.45 0.113 ## [90m 7[39m A 22.2 12.9 74 28.2 -[31m5[39m[31m.[39m[31m99[39m -[31m1[39m[31m.[39m[31m64[39m 0.083[4m7[24m 3.38 0.082[4m3[24m ## [90m 8[39m A 36.3 14.5 74 34.9 1.37 0.383 0.116 4.00 0.006[4m4[24m[4m3[24m ## [90m 9[39m A 38.3 16 72 40.8 -[31m2[39m[31m.[39m[31m45[39m -[31m0[39m[31m.[39m[31m787[39m 0.330 3.90 0.101 ## [90m10[39m A 55.7 17.5 82 49.6 6.13 1.87 0.255 3.16 0.397 ## [90m# … with 21 more rows[39m "],["more-of-base-and-stats.html", "3.3 More of {base} and {stats}", " 3.3 More of {base} and {stats} R’s {base} and {stats} libraries have lots of built-in functions that help perform statistical analysis. For example, anova() can be used to compare two regression models quickly. anova(reg_fit, poly_fit) ## Analysis of Variance Table ## ## Model 1: Volume ~ Girth + Height ## Model 2: Volume ~ Girth + I(Girth^2) + Height ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 28 421.92 ## 2 27 186.01 1 235.91 34.243 3.13e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We observe that the second order term for Girth does indeed provide significant explanatory power to the model. (Formally, we reject the null hypothesis that the second order term for Girth is zero.) What is ANOVA? Use base R statistical function when someone tries to test your statistics knowledge. Question: If \\(U_1\\) and \\(U_2\\) are i.i.d. \\(Unif(0,1)\\) random variables, what is the distribution of \\(U_1 + U_2\\)? set.seed(42) n &lt;- 10000 u_1 &lt;- runif(n) u_2 &lt;- runif(n) .hist &lt;- function(x, ...) { hist(x, probability = TRUE,...) lines(density(x), col = &quot;blue&quot;, lwd = 2, ...) } layout(matrix(c(1,2,3,3), 2, 2, byrow = TRUE)) .hist(u_1) .hist(u_2) .hist(u_1 + u_2) Answer: Evidently it’s triangular. There are probably lots of functions that you didn’t know you even needed. add_column &lt;- function(data) { # Whoops! `df` should be `data` df %&gt;% mutate(dummy = 1) } trees %&gt;% add_column() ## Error in UseMethod(&quot;mutate&quot;): no applicable method for &#39;mutate&#39; applied to an object of class &quot;function&quot; df() is the density function for the F distribution with df1 and df2 degrees of freedom df ## function (x, df1, df2, ncp, log = FALSE) ## { ## if (missing(ncp)) ## .Call(C_df, x, df1, df2, log) ## else .Call(C_dnf, x, df1, df2, ncp, log) ## } ## &lt;bytecode: 0x7f9b1cb0a518&gt; ## &lt;environment: namespace:stats&gt; "],["why-tidy-principles-and-tidymodels.html", "3.4 Why Tidy Principles and {tidymodels}?", " 3.4 Why Tidy Principles and {tidymodels}? The {tidyverse} has four guiding principles which {tidymodels} shares. It is human centered, i.e. the {tidyverse} is designed specifically to support the activities of a human data analyst. Functions use sensible defaults, or use no defaults in cases where the user must make a choice (e.g. a file path). {recipes} and {parnsip} enable data frames to be used every where in the modeling process. Data frames are often more convenient than working with matrices/vectors. It is consistent, so that what you learn about one function or package can be applied to another, and the number of special cases that you need to remember is as small as possible. Object orientated programming (mainly S3) for functions such as predict() provide a consistent interface to the user. broom::tidy() output is in a consistent format (data frame). List outputs provided by package-specific functions vary. It is composable, allowing you to solve complex problems by breaking them down into small pieces, supporting a rapid cycle of exploratory iteration to find the best solution. {recipes}, {parsnip}, {tune}, {dials}, etc are separate packages used in a tidy machine learning development workflow. It may seem inconvenient to have so many packages to perform specific tasks, but such a paradigm is helpful for decomposing the whole model design process, often making problems feel more manageable. It is inclusive, because the tidyverse is not just the collection of packages, but it is also the community of people who use them. Although the {tidyverse} and {tidymodels} are opinionated in their design, the developers are receptive to public feedback. "],["the-ames-housing-data.html", "Chapter 4 The Ames housing data", " Chapter 4 The Ames housing data Learning objectives: Explain when it makes sense to log-transform data. Explain why exploratory data analysis is an essential component of any modeling project. "],["slide-1-title.html", "4.1 Slide 1 Title", " 4.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title.html", "4.2 Slide 2 Title", " 4.2 Slide 2 Title Put the content of your second slide here. "],["spending-our-data.html", "Chapter 5 Spending our data", " Chapter 5 Spending our data Learning objectives: Jon will prefill these. "],["slide-1-title-1.html", "5.1 Slide 1 Title", " 5.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-1.html", "5.2 Slide 2 Title", " 5.2 Slide 2 Title Put the content of your second slide here. "],["feature-engineering-with-recipes.html", "Chapter 6 Feature engineering with recipes", " Chapter 6 Feature engineering with recipes Learning objectives: Jon will prefill these. "],["slide-1-title-2.html", "6.1 Slide 1 Title", " 6.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-2.html", "6.2 Slide 2 Title", " 6.2 Slide 2 Title Put the content of your second slide here. "],["fitting-models-with-parsnip.html", "Chapter 7 Fitting models with parsnip", " Chapter 7 Fitting models with parsnip Learning objectives: Jon will prefill these. "],["slide-1-title-3.html", "7.1 Slide 1 Title", " 7.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-3.html", "7.2 Slide 2 Title", " 7.2 Slide 2 Title Put the content of your second slide here. "],["a-model-workflow.html", "Chapter 8 A model workflow", " Chapter 8 A model workflow Learning objectives: Jon will prefill these. "],["slide-1-title-4.html", "8.1 Slide 1 Title", " 8.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-4.html", "8.2 Slide 2 Title", " 8.2 Slide 2 Title Put the content of your second slide here. "],["judging-model-effectiveness.html", "Chapter 9 Judging model effectiveness", " Chapter 9 Judging model effectiveness Learning objectives: Jon will prefill these. "],["slide-1-title-5.html", "9.1 Slide 1 Title", " 9.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-5.html", "9.2 Slide 2 Title", " 9.2 Slide 2 Title Put the content of your second slide here. "],["review-of-chapters-4-9.html", "Review of chapters 4-9", " Review of chapters 4-9 Learning objectives: Jon will prefill these. "],["slide-1-title-6.html", "9.3 Slide 1 Title", " 9.3 Slide 1 Title Put the content of your slide here. "],["slide-2-title-6.html", "9.4 Slide 2 Title", " 9.4 Slide 2 Title Put the content of your second slide here. "],["resampling-for-evaluating-performance.html", "Chapter 10 Resampling for evaluating performance", " Chapter 10 Resampling for evaluating performance Learning objectives: Jon will prefill these. "],["slide-1-title-7.html", "10.1 Slide 1 Title", " 10.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-7.html", "10.2 Slide 2 Title", " 10.2 Slide 2 Title Put the content of your second slide here. "],["comparing-models-with-resampling.html", "Chapter 11 Comparing models with resampling", " Chapter 11 Comparing models with resampling Learning objectives: Jon will prefill these. "],["slide-1-title-8.html", "11.1 Slide 1 Title", " 11.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-8.html", "11.2 Slide 2 Title", " 11.2 Slide 2 Title Put the content of your second slide here. "],["model-tuning-and-the-dangers-of-overfitting.html", "Chapter 12 Model tuning and the dangers of overfitting", " Chapter 12 Model tuning and the dangers of overfitting Learning objectives: Jon will prefill these. "],["slide-1-title-9.html", "12.1 Slide 1 Title", " 12.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-9.html", "12.2 Slide 2 Title", " 12.2 Slide 2 Title Put the content of your second slide here. "],["grid-search.html", "Chapter 13 Grid search", " Chapter 13 Grid search Learning objectives: Jon will prefill these. "],["slide-1-title-10.html", "13.1 Slide 1 Title", " 13.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-10.html", "13.2 Slide 2 Title", " 13.2 Slide 2 Title Put the content of your second slide here. "],["iterative-search.html", "Chapter 14 Iterative search", " Chapter 14 Iterative search Learning objectives: Jon will prefill these. "],["slide-1-title-11.html", "14.1 Slide 1 Title", " 14.1 Slide 1 Title Put the content of your slide here. "],["slide-2-title-11.html", "14.2 Slide 2 Title", " 14.2 Slide 2 Title Put the content of your second slide here. "],["review-of-chapters-10-14.html", "Review of chapters 10-14", " Review of chapters 10-14 Learning objectives: Jon will prefill these. "],["slide-1-title-12.html", "14.3 Slide 1 Title", " 14.3 Slide 1 Title Put the content of your slide here. "],["slide-2-title-12.html", "14.4 Slide 2 Title", " 14.4 Slide 2 Title Put the content of your second slide here. "]]
